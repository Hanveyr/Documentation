 ----LINUX I/O REDIRECTION, PIPES, AND TEES----

  TABLE OF CONTENTS
Introduction
Core Concepts and File Descriptors
Standard Streams
Redirection Operators
Advanced Redirection
Pipes and Command Chaining
The tee Command
Advanced Usage Patterns
Performance Considerations
Security Implications
Best Practices
Debugging and Troubleshooting
Real-World Examples
Shell-Specific Features
Conclusion





 1. INTRODUCTION
Linux I/O redirection is a fundamental feature providing precise control over command input and output streams. This guide covers all aspects of
I/O redirection, from basic concepts to advanced usage patterns.






-----I/O REDIRECTION AND PIPE FLOW DIAGRAMS-----

 1. STANDARD STREAMS (Default Configuration)
                +-------------+
Keyboard ------>| stdin         |
                | (0)           |
                +-------------+
                | PROCESS       |
                +-------------+
                | stdout        |---------> Terminal
                | (1)           |
                +-------------+
                | stderr        |---------> Terminal
                | (2)           |
                +-------------+


2. BASIC FILE REDIRECTION
                +-------------+
File.txt ------>| stdin     | < input.txt
                | (0)       |
                +-------------+
                | PROCESS   |
                +-------------+
                | stdout    |---------> output.txt > output.txt
                | (1)       |
                +-------------+
                | stderr    |---------> errors.log 2> errors.log
                | (2)       |
                +-------------+


3. PIPE CHAIN
Command1    Command2    Command3
+----------+ +----------+ +----------+
|           | |         | |          |
| Process1  | | Process2| | Process3 |
|           | |         | |          |
+----------+ +----------+ +----------+
        |            |           |
        | stdout     |   stdout  |  stdout
    +--------------->|--------------->|---------> Terminal
            pipe          pipe





  4. TEE COMMAND FLOW
                                    ┌──────────> File1.txt
                                    │
stdin ───────> tee command ─────────┼──────────> File2.txt
                                    │
                                    └──────────> Terminal




5. COMPLEX REDIRECTION (with stderr to stdout)
┌─────────────────────────────────────────────────────┐
│                                                     │
input.txt ──> stdin                                   │
              (fd 0)                                  │
                │                                     │
                v                                     │
          ┌───────────┐                              │
          │ PROCESS   │                              │
          └───────────┘                              │
                │                                     v
           ┌────┴────┐                          output.log
           │         │                               ▲
        stdout    stderr                            │
        (fd 1)    (fd 2) ──────────────────────────┘
           │
           v
        /dev/null




6. PIPELINE WITH ERROR HANDLING
┌─────────┐     ┌─────────┐     ┌─────────┐
│ Command1│ stdout│ Command2│ stdout│ Command3│
│         ├──────>│         ├──────>│         ├─────> output.txt
└────┬────┘      └────┬────┘      └────┬────┘
     │                │                 │
  stderr          stderr             stderr
     │                │                 │
     v                v                 v
errors1.log      errors2.log       errors3.log





7. PROCESS SUBSTITUTION
┌──────────────┐
│   Command1   │
│   <(cmd1)    │────┐
└──────────────┘    │
                    v
┌──────────────┐  ┌─────────┐
│   Command2   │  │         │
│   <(cmd2)    │──┤  diff   ├───> output
└──────────────┘  │         │
                  └─────────┘




8. HERE DOCUMENT
┌────────────────┐    ┌────────────┐
│ Here Document  │    │            │
│     << EOF     │───>│  Command   │───> output
│  (text block)  │    │            │
│      EOF       │    └────────────┘
└────────────────┘







LEGEND

----> Data flow direction
| Vertical data flow
+ Junction/split point
<> File descriptor
[] Process/command
() File descriptor number















Redirection Operators
Basic Output Redirection
1) Overwrite (>)

    Syntax: command > file
    Action: Overwrites existing content
    Example: ls > files.txt
    Warning: Destroys existing content
    Atomicity: Partial writes possible
    Permissions: Follows umask

2) Append (>>)

    Syntax: command >> file
    Action: Adds to existing content
    Example: echo "log" >> app.log
    Safety: Preserves existing data
    Atomicity: Append is atomic
    Use case: Logging systems

Error Stream Handling
1) Error to File (2>)

    Syntax: command 2> errors.log
    Purpose: Separate error capture
    Buffering: Immediate output
    Example: grep -R "pattern" * 2> errors.log
    Use case: Error logging

2) Error to Output (2>&1)

    Syntax: command > file 2>&1
    Order: Critical for correct operation
    Example: ls -l > all.log 2>&1
    Common mistakes: Wrong ordering
    Shell differences: Syntax variations

3) Combined Output (&>)

    Syntax: command &> file
    Modern equivalent: command > file 2>&1
    Shell support: Bash, Zsh
    Example: program &> output.log
    Convenience: Single operator

Input Redirection
1) From File (<)

    Syntax: command < input_file
    Purpose: File as input source
    Buffering: OS dependent
    Example: sort < data.txt
    Use case: Data processing

2) Here Document (<<)

    Syntax: command << DELIMITER
    Purpose: Inline multi-line input
    Example:
    bash

    cat << EOF
    line 1
    line 2
    EOF

    Escaping: Special character handling
    Indentation: Tab removal options

3) Here String (<<<)

    Syntax: command <<< "string"
    Purpose: Direct string input
    Example: grep "pattern" <<< "text"
    Efficiency: Avoids temp files
    Memory: Small string optimization

Special Redirections
1) Force Overwrite (>|)

    Syntax: command >| file
    Purpose: Override noclobber
    Example: echo "force" >| output.txt
    Safety: Bypasses protection
    Use case: Script requirements

2) Null Device (/dev/null)

    Syntax: command > /dev/null 2>&1
    Purpose: Discard all output
    Performance: Zero-cost output
    Use case: Unwanted output
    Special properties: Always success

3) Process Substitution

    Syntax: diff <(cmd1) <(cmd2)
    Purpose: Command output as files
    Implementation: Named pipes
    Example: diff <(sort file1) <(sort file2)
    Limitations: Not POSIX standard

This guide covers various types of redirection operators in Linux, including output redirection, error stream handling, input redirection, and special redirections. Each type is explained with examples and use cases to help you understand their applications and nuances.



























Understanding File Descriptors and Redirection in Linux
1. File Descriptors: The Basics

File descriptors (FDs) are non-negative integers that serve as handles to I/O resources in Linux. Think of them as identification numbers that the kernel uses to track open files and I/O streams.
Standard File Descriptors

Every process starts with three standard file descriptors:

    0 - stdin (Standard Input): Input to a command. Default is keyboard.
    1 - stdout (Standard Output): Output from a command. Default is terminal.
    2 - stderr (Standard Error): Error messages from a command. Default is terminal.

Example of viewing open file descriptors for a running process:
bash

# View FDs for a process with PID 1234
ls -l /proc/1234/fd

Additional File Descriptors

Processes can create additional file descriptors beyond the standard three:
bash

# Creating a new file descriptor (3) for writing
exec 3> output.log

# Writing to the new descriptor
echo "This goes to output.log" >&3

# Closing the descriptor when done
exec 3>&-

File Descriptor Limits

There are system-wide and per-process limits on file descriptors:
bash

# View current limits
ulimit -n

# View system-wide limits
cat /proc/sys/fs/file-max

# Increase limit for current session
ulimit -n 4096

2. Redirection Fundamentals
Basic Redirection Operations

Input Redirection (<):
bash

# Read from file instead of keyboard
grep "error" < logfile.txt

Output Redirection (>):
bash

# Write to file instead of screen
echo "Hello" > output.txt  # Overwrites file
echo "World" >> output.txt  # Appends to file

Error Redirection (2>):
bash

# Separate stdout and stderr
command > output.txt 2> errors.txt

Complex Redirection Examples

Combining stdout and stderr:
bash

# Method 1: Redirect stderr to stdout
command > output.txt 2>&1

# Method 2: Using newer syntax (bash)
command &> output.txt

Multiple Input/Output:
bash

# Read from one file, write to another
exec 3< input.txt 4> output.txt

while read -u 3 line; do
  echo "$line" >&4
done

Order Matters!
bash

# This works (stderr goes to file)
command 2>&1 > output.txt

# This doesn’t work as expected (stderr still goes to terminal)
command > output.txt 2>&1

3. Buffer Behavior and Race Conditions
Buffer Types

Line Buffering:
bash

# Force line buffering
stdbuf -oL command

No Buffering:
bash

# Disable buffering
stdbuf -o0 command

Race Condition Examples

Potential Race Condition:
bash

# This might have race conditions
echo "start" > file & echo "end" > file

# Safer approach using flock
(
  flock -x 200
  echo "start" > file
  echo "end" > file
) 200>file.lock

Preventing Race Conditions:
bash

# Using temporary files
temp1=$(mktemp)
temp2=$(mktemp)

command1 > "$temp1" &
command2 > "$temp2" &
wait

cat "$temp1" "$temp2" > final_output
rm "$temp1" "$temp2"

Best Practices

Always close unused file descriptors:
bash

# Good practice
exec 3> output.txt
echo "data" >&3
exec 3>&-

Use explicit error handling:
bash

if ! command > output.txt 2> error.txt; then
  echo "Command failed, check error.txt"
  exit 1
fi

Consider buffering when mixing stdout and stderr:
bash

# Force synchronization
{ command1; command2; } > output.txt 2>&1

4. Real-World Applications
Log Management:
bash

# Rotate logs while maintaining file descriptors
exec 3> service.log

while true; do
  echo "$(date): Service running" >&3
  if [ $(stat -c%s service.log) -gt 1048576 ]; then
    mv service.log service.log.old
    exec 3>&-
    exec 3> service.log
  fi
  sleep 60
done

Debug Output Control:
bash

# Enable/disable debug output dynamically
exec 5> debug.log
DEBUG_FD=5

debug() {
  if [ -n "$DEBUG_FD" ]; then
    echo "$@" >&$DEBUG_FD
  fi
}

# Usage
debug "Starting process"
command
debug "Process completed"

Multiple Output Streams:
bash

# Send different types of data to different files
exec 3> data.log
exec 4> errors.log
exec 5> metrics.log

log_data() { echo "$@" >&3; }
log_error() { echo "$@" >&4; }
log_metric() { echo "$@" >&5; }

# Usage
log_data "Data information"
log_error "Error information"
log_metric "Metric information"

Additional Advanced Examples:

Redirect stdout to a file and stderr to another command:
bash

command1 > output.log 2> >(command2)

Combining multiple redirections and pipes:
bash

(command1 && command2) | tee combined_output.txt > /dev/null

Using process substitution in a pipeline:
bash

diff <(command1) <(command2)

Conclusion

Understanding file descriptors and redirection is crucial for:

    Script debugging and logging
    Process automation
    System monitoring
    Data processing pipelines
    Service management
    Resource cleanup and management

Always consider:

    Resource limits and cleanup
    Buffer behavior
    Race conditions
    Error handling
    Performance implications











Named Pipes (FIFOs) and Advanced Descriptor Manipulation
1. Named Pipes (FIFOs)
Basic Creation and Usage

A named pipe (FIFO) allows for communication between processes. It is created using the mkfifo command.
bash

# Create a named pipe
mkfifo /tmp/mypipe

# Basic usage example
# Terminal 1
while true; do
  echo "$(date): Data written" > /tmp/mypipe
  sleep 1
done

# Terminal 2
while true; do
  if read line < /tmp/mypipe; then
    echo "Received: $line"
  fi
done













Basic Pipe Fundamentals
Simple Pipeline Examples
Basic Word Count Pipeline

This example shows how to count the number of words in a string using a pipeline.
bash

echo "hello world" | wc -w

Multiple Transformations

This example reads data from a file, sorts it, counts unique lines, and then sorts the results numerically in reverse order.
bash

cat data.txt | sort | uniq -c | sort -nr

Process Status Monitoring

This example lists all running processes, filters for processes related to nginx, and excludes the grep process itself from the results.
bash

ps aux | grep nginx | grep -v grep

Pipeline Exit Code Handling
Access to Pipeline Exit Codes

To properly handle exit codes in a pipeline, you can use set -o pipefail and check the status of each command in the pipeline.
bash

#!/bin/bash
# Enable pipeline exit code handling
set -o pipefail

# Example pipeline
command1 | command2 | command3

# Check pipeline status
echo "Pipeline status: ${PIPESTATUS[*]}"

# Check individual command statuses
echo "Command1: ${PIPESTATUS[0]}"
echo "Command2: ${PIPESTATUS[1]}"
echo "Command3: ${PIPESTATUS[2]}"

Advanced Pipeline Patterns
Parallel Processing Pipeline
Process Multiple Files in Parallel

This example processes multiple files in parallel, simulating a processing task for each file.
bash

#!/bin/bash

# Function to simulate file processing
process_file() {
  local file=$1
  sleep 1
  echo "Processed $file"
}
export -f process_file

# Process files in parallel with controlled output
find . -type f -name "*.txt" |
  parallel --max-procs=4 \
  --line-buffer \
  process_file {} 2>&1 |
  while IFS= read -r line; do
    echo "[$(date +%T)] $line"
  done

Error Handling in Pipelines
Robust Pipeline Error Handling

This script demonstrates robust error handling in a pipeline by using named pipes.
bash

#!/bin/bash

# Function to handle a pipeline with error handling
pipeline_with_errors() {
  local -a pids=()
  local -a pipes=()

  # Create pipes for error handling
  for i in {1..3}; do
    local pipe=$(mktemp -u)
    mkfifo "$pipe"
    pipes+=("$pipe")
  done

  # First command
  {
    command1 2>"${pipes[0]}" || echo "ERROR_CMD1" >&2
  } | \
  # Second command
  {
    command2 2>"${pipes[1]}" || echo "ERROR_CMD2" >&2
  } | \
  # Third command
  {
    command3 2>"${pipes[2]}" || echo "ERROR_CMD3" >&2
  } &

  # Collect PIDs
  pids=($(jobs -p))

  # Monitor errors
  for pipe in "${pipes[@]}"; do
    {
      while IFS= read -r line; do
        echo "Error: $line" >&2
      done < "$pipe"
      rm "$pipe"
    } &
  done

  # Wait for all processes
  for pid in "${pids[@]}"; do
    wait "$pid"
  done
}

# Example usage
pipeline_with_errors

Dead Pipe Detection
Detect and Handle Dead Pipes

This script detects and handles dead pipes in a pipeline, ensuring that both the reader and writer processes are monitored.
bash

#!/bin/bash

# Function to handle dead pipes
handle_dead_pipe() {
  local reader_pid=$1
  local writer_pid=$2

  while kill -0 "$reader_pid" 2>/dev/null && kill -0 "$writer_pid" 2>/dev/null; do
    sleep 0.1
  done

  # Check which process died
  if ! kill -0 "$reader_pid" 2>/dev/null; then
    echo "Reader died, killing writer" >&2
    kill "$writer_pid"
  elif ! kill -0 "$writer_pid" 2>/dev/null; then
    echo "Writer died, killing reader" >&2
    kill "$reader_pid"
  fi
}

# Usage example
{
  # Start reader
  while IFS= read -r line; do
    process_line "$line"
  done &
  reader_pid=$!

  # Start writer
  generate_data &
  writer_pid=$!

  # Monitor pipe health
  handle_dead_pipe "$reader_pid" "$writer_pid" &
  monitor_pid=$!

  # Wait for completion
  wait "$reader_pid" "$writer_pid" 2>/dev/null
  kill "$monitor_pid" 2>/dev/null
}




































Understanding File Descriptors and Redirection in Linux
1. Standard Input (stdin) - FD 0
Kernel-Level Implementation

File descriptors (FDs) are non-negative integers that serve as handles to I/O resources in Linux. The kernel uses these to track open files and I/O streams.
Standard File Descriptors

    0 - stdin (Standard Input): Input to a command. Default is keyboard.
    1 - stdout (Standard Output): Output from a command. Default is terminal.
    2 - stderr (Standard Error): Error messages from a command. Default is terminal.

Example of viewing open file descriptors for a running process:
bash

# View FDs for a process with PID 1234
ls -l /proc/1234/fd

Additional File Descriptors

Processes can create additional file descriptors beyond the standard three:
bash

# Creating a new file descriptor (3) for writing
exec 3> output.log

# Writing to the new descriptor
echo "This goes to output.log" >&3

# Closing the descriptor when done
exec 3>&-

File Descriptor Limits

There are system-wide and per-process limits on file descriptors:
bash

# View current limits
ulimit -n

# View system-wide limits
cat /proc/sys/fs/file-max

# Increase limit for current session
ulimit -n 4096

Redirection Fundamentals
Basic Redirection Operations

Input Redirection (<):
bash

# Read from file instead of keyboard
grep "error" < logfile.txt

Output Redirection (>):
bash

# Write to file instead of screen
echo "Hello" > output.txt  # Overwrites file
echo "World" >> output.txt  # Appends to file

Error Redirection (2>):
bash

# Separate stdout and stderr
command > output.txt 2> errors.txt

Complex Redirection Examples

Combining stdout and stderr:
bash

# Method 1: Redirect stderr to stdout
command > output.txt 2>&1

# Method 2: Using newer syntax (bash)
command &> output.txt

Multiple Input/Output:
bash

# Read from one file, write to another
exec 3< input.txt 4> output.txt

while read -u 3 line; do
  echo "$line" >&4
done

Order Matters!
bash

# This works (stderr goes to file)
command 2>&1 > output.txt

# This doesn’t work as expected (stderr still goes to terminal)
command > output.txt 2>&1

Advanced Buffering Mechanisms
Kernel Buffer Management
bash

# View current buffer settings
sysctl -a | grep -i buffer

# Modify kernel buffer size
sudo sysctl -w net.core.rmem_max=8388608

User-Space Buffer Control
bash

#!/bin/bash
# Custom buffer implementation with 64KB buffer size
exec 3< <(
  dd bs=65536 if=/dev/stdin 2>/dev/null | while IFS= read -r line; do
    echo "$line"
  done
)

Zero-Copy Operations
C

// Using splice() for zero-copy stdin operations
ssize_t bytes = splice(STDIN_FILENO, NULL, pipefd[1], NULL, 65536, SPLICE_F_MOVE);

Complete EOF State Machine
bash

#!/bin/bash
handle_eof() {
  local -i state=0
  while IFS= read -r -n1 char; do
    case $state in
      0) [[ $char == $'\004' ]] && ((state++)) ;;
      1) [[ $char == $'\004' ]] && return 0
         state=0 ;;
    esac
    echo -n "$char"
  done
  return 1
}

Advanced Blocking Control
Event-Driven Input
bash

#!/bin/bash
# Using epoll for non-blocking stdin
exec {EPOLL_FD}<> <(exec epoll_create1 0)
exec {STDIN_FD}<&0
epoll_ctl $EPOLL_FD ADD $STDIN_FD EPOLLIN

while :; do
  read -r event < <(exec epoll_wait $EPOLL_FD 1)
  [[ $event == " EPOLLIN " ]] && process_input
done

Timeout Hierarchy
bash

#!/bin/bash
# Implementing multi-level timeouts
read_with_timeout() {
  local timeout=$1
  local retry_count=$2
  local attempt=0

  while ((attempt < retry_count)); do
    if IFS= read -r -t "$timeout" line; then
      echo "$line"
      return 0
    fi
    ((attempt++))
    sleep 0.1
  done
  return 1
}

2. Standard Output (stdout) - FD 1
Kernel-Level Output Management
Page Cache Integration
C

// Understanding kernel page cache interaction
struct page *page;
char *buffer;
page = alloc_page(GFP_KERNEL);
buffer = page_address(page);
// Write to stdout through page cache

Write Barriers
bash

# Implementing write barriers
sync_output() {
  sync
  sysctl -w vm.drop_caches=3
}

Advanced Buffer Orchestration
Adaptive Buffering
bash

#!/bin/bash
# Dynamic buffer size adjustment
get_optimal_buffer_size() {
  local mem_available=$(free -b | awk '/Mem:/ {print $7}')
  local optimal_size=$(( mem_available / 10 ))
  echo $(( optimal_size > 65536 ? 65536 : optimal_size ))
}

BUFFER_SIZE=$(get_optimal_buffer_size)
dd bs=$BUFFER_SIZE if=/dev/stdin of=/dev/stdout

Buffer Chain Management
bash

#!/bin/bash
# Implementing buffer chains
buffer_chain() {
  local -a buffers=()
  local idx=0
  while IFS= read -r line; do
    buffers[idx++]=$line
    if ((idx >= 1000)); then
      printf '%s\n' "${buffers[@]}"
      idx=0
      buffers=()
    fi
  done
  # Flush remaining buffer
  [[ ${#buffers[@]} -gt 0 ]] && printf '%s\n' "${buffers[@]}"
}

Terminal Capability Integration
bash

# Complete terminal handling
setup_terminal() {
  if [[ -t 1 ]]; then
    # Get terminal capabilities
    local term_caps=$(tput -S <<< $'cup\nsmcup\nrmcup')
    # Configure based on capabilities
    stty -F /dev/tty -icanon min 1 time 0
    tput smcup # Save screen
    trap 'tput rmcup' EXIT
  fi
}

3. Standard Error (stderr) - FD 2
Kernel-Level Error Handling
Priority Management
C

// Priority-based error routing
struct error_priority {
  int level;
  int facility;
  unsigned long flags;
};

Error Stream Atomicity
bash

# Atomic error writing
write_atomic_error() {
  local message=$1
  {
    flock -x 200
    echo "$message" >&2
  } 200>/dev/stderr.lock
}

Advanced Error Stream Patterns
Structured Error Output
bash

#!/bin/bash
# JSON-structured errors
error_json() {
  local level=$1
  local code=$2
  local message=$3
  jq -nc \
    --arg level "$level" \
    --arg code "$code" \
    --arg msg "$message" \
    --arg timestamp "$(date -Iseconds)" \
    '{level: $level, code: $code, message: $msg, timestamp: $timestamp}' >&2
}

Error Stream Multiplexing
bash

#!/bin/bash
# Multi-destination error routing
route_error() {
  local message=$1
  tee \
    >(logger -t myapp -p error) \
    >(aws cloudwatch put-log-events --log-group-name myapp) \
    >/dev/stderr <<< "$message"
}

Real-Time Error Analytics
bash

#!/bin/bash
# Error pattern detection
analyze_errors() {
  local window=60
  local threshold=10
  tail -f /dev/stderr |
  grep -v '^$' |
  awk -v window="$window" -v threshold="$threshold" '
  {
    current_time = systime()
    errors[current_time]++
    for (t in errors)
      if (current_time - t > window)
        delete errors[t]
    total = 0
    for (t in errors)
      total += errors[t]
    if (total >= threshold)
      system("notify-send \"Error threshold exceeded!\"")
  }'
}

This comprehensive guide covers the basics and advanced concepts of file descriptors and redirection in Linux, including real-world applications and best practices.


























Linux tee Command
1. Basic Usage Patterns
Simple Stream Splitting

The tee command reads from standard input and writes to standard output and files.
bash

# Basic usage
echo "Hello, World!" | tee output.txt

# Multiple outputs
echo "Multi-file output" | tee file1.txt file2.txt file3.txt

# Combining with other commands
cat input.txt | grep "error" | tee errors.log | wc -l

Buffer Control

This script demonstrates controlled buffering with tee to manage the flow of data.
bash

#!/bin/bash

# Function for controlled buffering with tee
handle_buffered_output() {
  local buffer_size=4096

  # Force buffer size using dd
  dd bs=$buffer_size 2>/dev/null | \
  tee >(dd bs=$buffer_size of=output1.txt 2>/dev/null) \
      >(dd bs=$buffer_size of=output2.txt 2>/dev/null)
}

# Generate data with controlled buffer
generate_data | handle_buffered_output

2. Error Management
Robust Error Handling

This script demonstrates advanced error handling for tee operations by capturing errors and handling them appropriately.
bash

#!/bin/bash

# Function for advanced error handling with tee
tee_with_errors() {
  local -a output_files=("$@")
  local -a pipes=()
  local -a pids=()

  # Create error capture pipes
  for ((i=0; i<${#output_files[@]}; i++)); do
    local pipe=$(mktemp -u)
    mkfifo "$pipe"
    pipes+=("$pipe")
  done

  # Start error monitors
  for pipe in "${pipes[@]}"; do
    {
      while IFS= read -r error; do
        echo "Tee error: $error" >&2
      done < "$pipe"
      rm "$pipe"
    } &
    pids+=($!)
  done

  # Run tee with error redirection
  tee "${output_files[@]}" 2> >(tee "${pipes[@]}" >/dev/null)

  # Cleanup
  wait "${pids[@]}" 2>/dev/null
}

# Usage
echo "Test data" | tee_with_errors output1.txt output2.txt

3. Signal Handling
Advanced Signal Management

This script demonstrates comprehensive signal handling for tee operations, ensuring clean-up on receiving signals.
bash

#!/bin/bash

# Function for tee with comprehensive signal handling
tee_with_signals() {
  local -a files=("$@")
  local -a pids=()

  cleanup() {
    local signal=$1
    echo "Received signal: $signal" >&2

    # Kill all child processes
    for pid in "${pids[@]}"; do
      kill -TERM "$pid" 2>/dev/null
    done

    # Remove incomplete output files
    for file in "${files[@]}"; do
      rm -f "$file.incomplete"
    done

    exit 1
  }

  # Set up signal handlers
  trap 'cleanup SIGHUP' SIGHUP
  trap 'cleanup SIGINT' SIGINT
  trap 'cleanup SIGTERM' SIGTERM

  # Start tee processes with temporary files
  for file in "${files[@]}"; do
    tee "$file.incomplete" &
    pids+=($!)
  done

  # Wait for completion
  wait "${pids[@]}"

  # Atomic rename of completed files
  for file in "${files[@]}"; do
    mv "$file.incomplete" "$file"
  done
}

# Usage
generate_data | tee_with_signals output1.txt output2.txt

4. Atomic Operations
Atomic Write Patterns

This script demonstrates atomic write operations with tee to ensure file integrity.
bash

#!/bin/bash

# Function for atomic tee operations
atomic_tee() {
  local -a files=("$@")
  local -a temp_files=()
  local -a locks=()

  # Create temporary files and locks
  for file in "${files[@]}"; do
    temp_files+=("$file.tmp.$$")
    locks+=("$file.lock")
  done

  # Acquire locks
  for lock in "${locks[@]}"; do
    exec {lock_fd}>"$lock"
    flock -x "$lock_fd"
  done

  # Write to temporary files
  tee "${temp_files[@]}"

  # Atomic rename
  for ((i=0; i<${#files[@]}; i++)); do
    mv "${temp_files[i]}" "${files[i]}"
  done

  # Release locks
  for lock in "${locks[@]}"; do
    rm -f "$lock"
  done
}

# Usage
echo "Atomic write" | atomic_tee file1.txt file2.txt




















1. Stdout/Stderr Separation
Basic Pattern

This pattern shows how to separate stdout and stderr into different log files, with the addition of timestamps.
bash

# Basic separation
command > stdout.log 2> stderr.log

# With timestamp addition
command \
1> >(ts '[%Y-%m-%d %H:%M:%S]' >> stdout.log) \
2> >(ts '[%Y-%m-%d %H:%M:%S]' >> stderr.log)

Advanced Implementation

This script demonstrates an advanced implementation for separating stdout and stderr, including timestamps and atomic writes using named pipes.
bash

#!/bin/bash
log_with_separation() {
  local cmd="$1"
  local stdout_file="$2"
  local stderr_file="$3"

  # Create named pipes for atomic writes
  local stdout_pipe=$(mktemp -u)
  local stderr_pipe=$(mktemp -u)
  mkfifo "$stdout_pipe" "$stderr_pipe"

  # Start background logging processes
  {
    while IFS= read -r line; do
      printf '[%s] %s\n' "$(date -u '+%Y-%m-%d %H:%M:%S')" "$line"
    done < "$stdout_pipe" >> "$stdout_file"
  } &
  local stdout_pid=$!

  {
    while IFS= read -r line; do
      printf '[%s] ERROR: %s\n' "$(date -u '+%Y-%m-%d %H:%M:%S')" "$line"
    done < "$stderr_pipe" >> "$stderr_file"
  } &
  local stderr_pid=$!

  # Execute command with redirection
  eval "$cmd" > "$stdout_pipe" 2> "$stderr_pipe"

  # Cleanup
  rm -f "$stdout_pipe" "$stderr_pipe"
  wait "$stdout_pid" "$stderr_pid"
}

# Usage
log_with_separation \
  "find /etc -type f" \
  "find_output.log" \
  "find_errors.log"

2. Pipeline Error Handling
Basic Pattern

This pattern shows how to handle errors in a pipeline by redirecting stderr to stdout and then to /dev/null, or capturing errors in a log file.
bash

# Redirect stderr to stdout, then to /dev/null
command1 2>&1 >/dev/null | command2

# Capture all errors while processing output
command1 2>&1 >/dev/null | command2 2> pipeline_errors.log

Advanced Implementation

This script demonstrates advanced pipeline error handling, capturing errors from each command in the pipeline.
bash

#!/bin/bash
pipeline_with_error_handling() {
  local -a commands=("$@")
  local -a pipes=()
  local -a pids=()

  # Set up error handling
  set -o pipefail

  # Create error capture pipes
  for ((i=0; i<${#commands[@]}; i++)); do
    local pipe=$(mktemp -u)
    mkfifo "$pipe"
    pipes+=("$pipe")
  done

  # Error monitoring function
  monitor_errors() {
    local pipe=$1
    local cmd=$2
    local index=$3

    while IFS= read -r error; do
      printf "Error in command %d (%s): %s\n" \
        "$index" "$cmd" "$error" >&2
    done < "$pipe"
  }

  # Start error monitors
  for i in "${!commands[@]}"; do
    monitor_errors "${pipes[i]}" "${commands[i]}" "$i" &
    pids+=($!)
  done

  # Execute pipeline
  {
    for i in "${!commands[@]}"; do
      if ((i == 0)); then
        eval "${commands[i]} 2> '${pipes[i]}'"
      else
        echo "| ${commands[i]} 2> '${pipes[i]}'"
      fi
    done
  } | bash

  local status=$?

  # Cleanup
  wait "${pids[@]}" 2>/dev/null
  rm -f "${pipes[@]}"

  return $status
}

# Usage
pipeline_with_error_handling \
  "find /etc -type f" \
  "grep 'config'" \
  "sort -u"

3. Bidirectional Pipes (Coprocess)
Basic Pattern

This pattern shows basic usage of a coprocess for bidirectional communication between commands.
bash

# Basic coprocess usage
coproc { command1; }
command2 >&"${COPROC[1]}" <&"${COPROC[0]}"

Advanced Implementation

This script demonstrates advanced bidirectional communication with a coprocess, including timeout handling.
bash

#!/bin/bash
bidirectional_communication() {
  local producer="$1"
  local consumer="$2"
  local timeout=${3:-30}

  # Start coprocess
  coproc PROC {
    eval "$producer"
  }

  # Setup timeout handler
  handle_timeout() {
    echo "Operation timed out" >&2
    kill "$PROC_PID" 2>/dev/null
    exit 1
  }

  trap 'handle_timeout' SIGALRM

  # Buffer for collecting output
  local buffer=""
  local chunk

  # Read from coprocess
  while true; do
    if ! read -t "$timeout" -r chunk <&"${PROC[0]}"; then
      if [[ $? -eq 142 ]]; then # Timeout
        handle_timeout
      fi
      break
    fi
    buffer+="$chunk"$'\n'

    # Process accumulated data
    if [[ $buffer =~ \$ ]]; then # End marker
      # Remove end marker
      buffer=${buffer%$'\n'}

      # Process with consumer
      eval "$consumer" <<< "$buffer"
      buffer=""
    fi
  done

  # Cleanup
  exec {PROC[0]}>&-
  exec {PROC[1]}>&-
  wait "$PROC_PID" 2>/dev/null
}

# Example usage
producer_cmd='for i in {1..5}; do
  echo "Data $i"
  sleep 1
done
echo "$"'

consumer_cmd='while IFS= read -r line; do
  echo "Processed: $line"
done'

bidirectional_communication "$producer_cmd" "$consumer_cmd" 10

1 vulnerability detected

This comprehensive guide covers various advanced redirection patterns and techniques in Unix/Linux, including stdout/stderr separation, pipeline error handling, and bidirectional pipes with coprocess communication. These examples demonstrate how to effectively manage and handle data streams and errors in Unix/Linux shell scripting.
























































Advanced Integration Patterns
Stream Synchronization

Stream synchronization ensures that output and error streams are perfectly synchronized, preventing race conditions and ensuring atomic writes.
Perfect Stream Synchronization
bash

sync_streams() {
  local tmp_out=$(mktemp)
  local tmp_err=$(mktemp)
  mkfifo "$tmp_out" "$tmp_err"

  exec 3>"$tmp_out" 4>"$tmp_err"

  # Atomic write to both streams
  (
    flock -x 200
    echo "stdout data" >&3
    echo "stderr data" >&4
  ) 200>/dev/streams.lock
}

Zero-Copy Stream Operations

Zero-copy operations minimize CPU usage by eliminating the need to copy data between buffers.
Implementation of Zero-Copy Between Streams
bash

zero_copy_transfer() {
  local src_fd=$1
  local dst_fd=$2
  local buffer_size=65536

  while :; do
    splice "$src_fd" /dev/null \
           "$dst_fd" /dev/null \
           "$buffer_size" \
           SPLICE_F_MOVE
  done
}

Cross-Platform Stream Handling

Managing streams across different platforms requires adapting to platform-specific APIs.
Platform-Adaptive Stream Management
bash

setup_streams() {
  case "$(uname)" in
    Linux)
      use_epoll
      ;;
    Darwin)
      use_kqueue
      ;;
    *)
      use_select
      ;;
  esac
}

Performance Optimization

Optimizing stream handling for performance involves leveraging advanced techniques like memory-mapped I/O and real-time compression.
Memory-Mapped I/O Integration

Memory-mapped I/O can significantly improve the performance of file operations by mapping files directly into memory.
bash

mmap_stream() {
  local file=$1
  python3 -c "
import mmap
with open('$file', 'rb') as f:
    mm = mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ)
    while True:
        line = mm.readline()
        if not line: break
        print(line.decode().rstrip())
"
}

Stream Compression Pipeline

Real-time stream compression reduces the size of data streams on-the-fly, which is useful for bandwidth-constrained environments.
bash

compress_stream() {
  local compression_level=${1:-9}
  local tmp_fifo=$(mktemp -u)
  mkfifo "$tmp_fifo"

  # Parallel compression pipeline
  zstd -$compression_level - \
    < "$tmp_fifo" \
    > >(split -b 64M - compressed_chunk_) &

  cat > "$tmp_fifo"
  rm "$tmp_fifo"
}

Summary

These advanced integration patterns provide powerful tools for managing and optimizing data streams in Linux. By using techniques like perfect synchronization, zero-copy operations, and memory-mapped I/O, you can achieve high-performance and cross-platform compatibility in your applications.











































Advanced FIFO Patterns
Non-blocking Operations

Non-blocking operations prevent processes from being stuck waiting for an I/O operation to complete.
bash

#!/bin/bash
# Create pipe with error handling
if ! mkfifo -m 600 /tmp/mypipe 2>/dev/null; then
  echo "Error creating pipe, checking if exists..."
  [[ -p /tmp/mypipe ]] || { echo "Fatal error"; exit 1; }
fi

# Non-blocking write with timeout
write_with_timeout() {
  local timeout=$1
  local data=$2
  local pipe=$3

  # Open pipe for writing without blocking
  exec 3<> "$pipe"

  if flock -w "$timeout" -x 3; then
    echo "$data" >&3
    exec 3>&-
    return 0
  else
    exec 3>&-
    return 1
  fi
}

# Usage
write_with_timeout 5 "Test data" /tmp/mypipe

Buffer Management

Managing buffer size can optimize performance and efficiency.
bash

#!/bin/bash
# Configure pipe buffer size
pipe="/tmp/mypipe"
mkfifo "$pipe"

# Get current pipe buffer size
pipe_size=$(stat -f %z "$pipe" 2>/dev/null || stat -c %s "$pipe")

# Set optimal buffer size
if [[ -w /proc/sys/fs/pipe-max-size ]]; then
  optimal_size=$((64*1024)) # 64KB
  echo "$optimal_size" > /proc/sys/fs/pipe-max-size
fi

Race Condition Prevention

Prevent race conditions using atomic operations and locking mechanisms.
bash

#!/bin/bash
# Atomic write to pipe with locking
PIPE="/tmp/mypipe"
LOCK="/tmp/mypipe.lock"

atomic_write() {
  local data=$1
  (
    # Acquire exclusive lock
    flock -x 200
    # Write data atomically
    echo "$data" > "$PIPE"
  ) 200>"$LOCK"
}

# Multiple process safety
atomic_write "Process 1 data" &
atomic_write "Process 2 data" &
wait

2. Advanced Descriptor Manipulation
Descriptor State Management
Saving and Restoring Output Streams
bash

#!/bin/bash
# Save original stdout
exec 3>&1
# Redirect stdout to a file
exec 1>output.log
# Do some work
echo "This goes to the file"
# Restore original stdout
exec 1>&3
echo "This goes to the terminal"
# Clean up
exec 3>&-

Multiple Descriptor Management
bash

#!/bin/bash
# Advanced descriptor juggling
setup_descriptors() {
  # Save original descriptors
  exec 3>&1  # Save stdout
  exec 4>&2  # Save stderr

  # Create new log files
  exec 1>>"$LOG_FILE"
  exec 2>>"$ERROR_LOG"

  # Create custom descriptor for debug output
  exec 5>>"$DEBUG_LOG"
}

cleanup_descriptors() {
  # Restore original descriptors
  exec 1>&3
  exec 2>&4

  # Close saved descriptors
  exec 3>&-
  exec 4>&-
  exec 5>&-
}

# Usage
LOG_FILE="app.log"
ERROR_LOG="error.log"
DEBUG_LOG="debug.log"
setup_descriptors
trap cleanup_descriptors EXIT

Temporary Redirection Pattern
bash

#!/bin/bash
# Function for temporary redirection
with_redirect() {
  local outfile=$1
  local errfile=$2
  shift 2

  # Save current descriptors
  exec 3>&1 4>&2
  # Set up new descriptors
  exec 1>"$outfile" 2>"$errfile"

  # Run command
  "$@"
  local status=$?

  # Restore descriptors
  exec 1>&3 2>&4
  # Cleanup
  exec 3>&- 4>&-
  return $status
}

# Usage
with_redirect "output.log" "error.log" some_command arg1 arg2

Scope Management
Subshell Descriptor Isolation
bash

#!/bin/bash
# Demonstrate descriptor isolation
exec 3>custom.log
echo "Main shell" >&3

(
  # This subshell gets its own copy of descriptor 3
  echo "Subshell 1" >&3
  exec 3>different.log
  echo "Redirected in subshell" >&3
)

# Original descriptor 3 is unchanged in main shell
echo "Still main shell" >&3

Function Descriptor Handling
bash

#!/bin/bash
# Function with local descriptor management
process_with_logs() {
  local log=$1
  shift

  # Local descriptor
  exec {log_fd}>"$log"
  echo "Starting process at $(date)" >&$log_fd
  "$@" >&$log_fd
  echo "Finished at $(date)" >&$log_fd
  # Clean up local descriptor
  exec {log_fd}>&-
}

# Usage
process_with_logs "process.log" long_running_command arg1 arg2

Cleanup Patterns
Comprehensive Cleanup
bash

#!/bin/bash
# Setup cleanup handling
declare -a PIPES=()
declare -a FDS=()

cleanup() {
  # Close all tracked file descriptors
  for fd in "${FDS[@]}"; do
    exec {fd}>&- 2>/dev/null
  done

  # Remove all tracked pipes
  for pipe in "${PIPES[@]}"; do
    rm -f "$pipe" 2>/dev/null
  done
}

create_pipe() {
  local pipe=$(mktemp -u)
  mkfifo "$pipe"
  PIPES+=("$pipe")
  echo "$pipe"
}

open_fd() {
  local target=$1
  exec {new_fd}>"$target"
  FDS+=($new_fd)
  echo "$new_fd"
}

# Set up cleanup trap
trap cleanup EXIT INT TERM

This guide covers the creation and usage of named pipes (FIFOs) and advanced file descriptor manipulation techniques in bash. It includes basic examples as well as advanced patterns for non-blocking operations, buffer management, race condition prevention, and comprehensive cleanup.






















Buffer Management
Prevent Buffer Blockage

This script manages pipeline buffers to prevent blockage by controlling the buffer size.
bash

#!/bin/bash

# Function to manage pipeline buffers
manage_pipeline_buffers() {
  local buffer_size=4096
  local tmp_file=$(mktemp)

  # Writer with controlled buffer
  {
    while IFS= read -r data; do
      # Process in chunks
      {
        echo "$data"
        sleep 0.1 # Simulate processing
      } > "$tmp_file"

      # Control buffer size
      dd bs=$buffer_size if="$tmp_file" 2>/dev/null
    done < <(generate_large_data)
  } | \
  # Reader with controlled consumption
  {
    while IFS= read -r -n $buffer_size chunk; do
      process_chunk "$chunk"
    done
  }

  rm -f "$tmp_file"
}

# Example usage
manage_pipeline_buffers

This guide covers basic and advanced concepts of using pipes in bash scripts, including simple pipelines, handling exit codes, parallel processing, robust error handling, dead pipe detection, and buffer management. Each example includes explanations and use cases to help you understand their applications and nuances.


















Resource Management
Memory-Efficient Pipelines
Memory-Efficient Large File Processing

This script processes a large file in chunks to minimize memory usage, showing progress in the process.
bash

#!/bin/bash

# Function to process a large file in memory-efficient chunks
process_large_file() {
  local input=$1
  local chunk_size=1048576 # 1MB chunks

  # Process in chunks with progress
  {
    local size=$(stat -c %s "$input")
    local processed=0

    dd if="$input" bs=$chunk_size 2>/dev/null |
    while IFS= read -r -n $chunk_size chunk; do
      processed=$((processed + ${#chunk}))
      printf "Progress: %d%%\r" $((processed * 100 / size)) >&2
      echo "$chunk"
    done
  } | \
  process_data | \
  compress_output > "${input}.processed"
}

# Dummy functions for the example
process_data() {
  while IFS= read -r line; do
    echo "Processed: $line"
  done
}

compress_output() {
  gzip
}

# Example usage
process_large_file "largefile.txt"

In this script, process_large_file reads the input file in 1MB chunks using dd and processes each chunk. The progress is displayed as a percentage. The processed data is then passed to process_data and compress_output before being saved to a new file.
Pipeline Resource Monitoring
Monitor Pipeline Resource Usage

This script monitors the memory and CPU usage of processes in a pipeline.
bash

#!/bin/bash

# Function to monitor pipeline resource usage
monitor_pipeline() {
  local pids=("$@")
  local interval=1

  while true; do
    for pid in "${pids[@]}"; do
      if ! kill -0 "$pid" 2>/dev/null; then
        continue
      fi

      # Get resource usage
      local mem=$(ps -o rss= -p "$pid" 2>/dev/null)
      local cpu=$(ps -o %cpu= -p "$pid" 2>/dev/null)

      echo "[PID $pid] MEM: ${mem}KB CPU: ${cpu}%"
    done

    # All processes finished
    if ! kill -0 "${pids[@]}" 2>/dev/null; then
      break
    fi

    sleep "$interval"
  done
}

# Example usage
command1 | command2 | command3 &
pids=($(jobs -p))
monitor_pipeline "${pids[@]}" &
monitor_pid=$!

wait "${pids[@]}"
kill "$monitor_pid" 2>/dev/null

In this script, monitor_pipeline takes a list of PIDs and monitors their memory and CPU usage at regular intervals until all processes are finished.
Exit Code Coordination
Advanced Exit Status Handling
Coordinate Multiple Pipeline Exit Codes

This script coordinates the exit statuses of multiple commands in a pipeline, determining the most severe error.
bash

#!/bin/bash

# Function to handle multiple pipeline exit codes
handle_pipeline_status() {
  local -a statuses=("${PIPESTATUS[@]}")
  local -a names=("$@")
  local final_status=0

  for i in "${!statuses[@]}"; do
    local status=${statuses[i]}
    local name=${names[i]:-"Command$((i+1))"}

    if ((status != 0)); then
      echo "$name failed with status $status" >&2
      # Determine most severe error
      ((status > final_status)) && final_status=$status
    fi
  done

  return $final_status
}

# Example usage
process_data | filter_results | generate_report
handle_pipeline_status "Processor" "Filter" "Generator"

In this script, handle_pipeline_status evaluates the exit statuses of commands in a pipeline, printing an error message for any command that fails and returning the most severe error status.

These scripts demonstrate memory-efficient processing, resource monitoring, and advanced exit status handling in pipelines. By processing files in chunks, monitoring resource usage, and coordinating exit codes, you can create robust and efficient pipeline workflows.






















5. Advanced Features
Append Mode with Verification

This script demonstrates an advanced append mode with verification to ensure data integrity.
bash

#!/bin/bash

# Function for advanced append mode with verification
verified_tee_append() {
  local file=$1
  local temp=$(mktemp)
  local lock="$file.lock"

  # Acquire lock
  exec {lock_fd}>"$lock"
  flock -x "$lock_fd"

  # Capture input
  tee -a "$temp"

  # Verify write
  if ! cmp -s "$temp" <(tail -c "$(stat -c %s "$temp")" "$file"); then
    cat "$temp" >> "$file"
  fi

  rm -f "$temp" "$lock"
}

# Usage
echo "Appending data" | verified_tee_append output.log

Multiple Target Management

This script demonstrates advanced handling of multiple targets for tee.
bash

#!/bin/bash

# Function for advanced multiple target handling
multi_target_tee() {
  local -a targets=()
  local buffer_size=4096

  # Parse targets with options
  while [[ $# -gt 0 ]]; do
    case $1 in
      --file=*)
        targets+=("${1#*=}")
        ;;
      --pipe=*)
        local pipe="${1#*=}"
        mkfifo "$pipe"
        targets+=("$pipe")
        ;;
      --process=*)
        local cmd="${1#*=}"
        targets+=(">($cmd)")
        ;;
    esac
    shift
  done

  # Create tee command with all targets
  local cmd="tee"
  for target in "${targets[@]}"; do
    cmd="$cmd $target"
  done

  # Execute with controlled buffering
  eval "dd bs=$buffer_size 2>/dev/null | $cmd"
}

# Usage
generate_data | multi_target_tee \
  --file=output1.txt \
  --pipe=named_pipe \
  --process="grep error > errors.log"

6. Buffer Management
Adaptive Buffer Control

This script demonstrates adaptive buffer management for tee to optimize performance based on available memory.
bash

#!/bin/bash

# Function for adaptive buffer management with tee
adaptive_tee() {
  local -a files=("$@")
  local mem_available=$(free -b | awk '/Mem:/ {print $7}')
  local optimal_buffer=$((mem_available / 100)) # Use 1% of available memory

  # Cap buffer size
  (( optimal_buffer > 1048576 )) && optimal_buffer=1048576 # Max 1MB

  # Create buffer controlled tee pipeline
  dd bs=$optimal_buffer 2>/dev/null | \
  tee $(printf -- ">( dd bs=%d of='%s' 2>/dev/null) " \
    "$optimal_buffer" "${files[@]}")
}

# Usage
cat large_file | adaptive_tee output1.txt output2.txt

This comprehensive guide covers various advanced usage patterns of the tee command, including error management, signal handling, atomic operations, and buffer management. These examples demonstrate how to leverage tee for robust and efficient data processing workflows in Linux.
























1. Stdout/Stderr Separation
Basic Separation

This script demonstrates the separation of stdout and stderr with metadata logging.
bash

#!/bin/bash
# Complete separation with metadata
log_with_separation() {
  local cmd=$1
  local stdout_log=$2
  local stderr_log=$3
  local meta_log=$4

  # Start timestamp
  echo "Command: $cmd" > "$meta_log"
  echo "Start: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> "$meta_log"

  # Execute with separated streams
  TIMEFORMAT='%R seconds'
  time (
    $cmd > >(
      while IFS= read -r line; do
        echo "[$(date +%T)] $line" >> "$stdout_log"
      done
    ) 2> >(
      while IFS= read -r line; do
        echo "[$(date +%T)] $line" >> "$stderr_log"
      done
    )
  ) 2>> "$meta_log"

  # End timestamp
  echo "End: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> "$meta_log"
}

# Usage
log_with_separation "find / -type f" \
"find_output.log" \
"find_errors.log" \
"find_meta.log"

Advanced Stream Management

This script demonstrates multi-level stream separation with prioritized logging.
bash

#!/bin/bash
# Multi-level stream separation
advanced_stream_separation() {
  local cmd=$1
  shift
  local -a outputs=("$@")

  # Create FIFOs for each priority level
  local -a fifos=()
  for i in "${!outputs[@]}"; do
    local fifo=$(mktemp -u)
    mkfifo "$fifo"
    fifos+=("$fifo")
  done

  # Start logging processes
  for i in "${!outputs[@]}"; do
    {
      while IFS= read -r line; do
        echo "[PRIORITY$i] $line" >> "${outputs[i]}"
      done < "${fifos[i]}"
    } &
  done

  # Execute command with priority routing
  eval "$cmd" \
    1> "${fifos[0]}" \
    2> >(
      while IFS= read -r line; do
        if [[ $line == *"ERROR"* ]]; then
          echo "$line" > "${fifos[2]}"
        else
          echo "$line" > "${fifos[1]}"
        fi
      done
    )

  # Cleanup
  rm -f "${fifos[@]}"
}

# Usage
advanced_stream_separation "your_command" "output1.log" "output2.log" "error.log"

2. Pipeline Error Handling
Robust Pipeline Error Management

This script demonstrates advanced error handling in a pipeline, capturing errors from each command.
bash

#!/bin/bash
# Advanced pipeline error handling
handle_pipeline_errors() {
  local -a commands=("$@")
  local -a pipes=()
  local -a pids=()

  # Create error capture pipes
  for ((i=0; i<${#commands[@]}; i++)); do
    local pipe=$(mktemp -u)
    mkfifo "$pipe"
    pipes+=("$pipe")
  done

  # Start error monitors
  for i in "${!pipes[@]}"; do
    {
      while IFS= read -r error; do
        echo "Error in ${commands[i]}: $error" >&2
      done < "${pipes[i]}"
    } &
    pids+=($!)
  done

  # Execute pipeline with error redirection
  {
    for i in "${!commands[@]}"; do
      if ((i == 0)); then
        eval "${commands[i]} 2> '${pipes[i]}'"
      else
        eval "${commands[i]} 2> '${pipes[i]}'"
      fi
      if ((i < ${#commands[@]} - 1)); then
        echo "|"
      fi
    done
  } | bash

  # Cleanup
  wait "${pids[@]}" 2>/dev/null
  rm -f "${pipes[@]}"
}

# Usage
handle_pipeline_errors \
  "find / -type f" \
  "grep error" \
  "sort -u"

Error Recovery Patterns

This script demonstrates error recovery in a pipeline, retrying failed commands.
bash

#!/bin/bash
# Pipeline with error recovery
recoverable_pipeline() {
  local max_retries=3
  local retry_delay=5

  pipeline_with_retry() {
    local attempt=1
    local success=false

    while ((attempt <= max_retries)) && ! $success; do
      if eval "$1" 2>/dev/null; then
        success=true
      else
        echo "Attempt $attempt failed, retrying in $retry_delay seconds..." >&2
        sleep "$retry_delay"
        ((attempt++))
      fi
    done

    $success
  }

  # Execute pipeline with recovery
  pipeline_with_retry "command1" |
  while IFS= read -r line; do
    if ! pipeline_with_retry "command2 <<< '$line'"; then
      echo "$line" >> failed_items.log
    fi
  done
}

3. Bidirectional Pipes (Coprocess)
Advanced Coprocess Communication

This script demonstrates bidirectional communication with a coprocess.
bash

#!/bin/bash
# Bidirectional communication manager
bidirectional_manager() {
  local input_handler=$1
  local output_handler=$2

  # Start coprocess
  coproc handler {
    while IFS= read -r line; do
      eval "$input_handler '$line'" || break
    done
  }

  # Setup communication channels
  exec 3>&"${COPROC[1]}" # Write to coprocess
  exec 4<&"${COPROC[0]}" # Read from coprocess

  # Main processing loop
  while IFS= read -r line; do
    echo "$line" >&3 # Send to coprocess
    if IFS= read -r response <&4; then
      eval "$output_handler '$response'"
    fi
  done

  # Cleanup
  exec 3>&- 4<&-
  wait "$COPROC_PID" 2>/dev/null
}

# Usage example
process_input() {
  local input=$1
  echo "Processing: $input"
}

handle_output() {
  local output=$1
  echo "Result: $output"
}

bidirectional_manager process_input handle_output

1 vulnerability detected

Interactive Coprocess Pattern

This script demonstrates interactive bidirectional communication with a coprocess.
bash

#!/bin/bash
# Interactive bidirectional communication
interactive_coprocess() {
  local timeout=30

  # Start interactive process
  coproc interactive {
    while true; do
      read -r cmd
      case "$cmd" in
        "exit") break ;;
        *) eval "$cmd" ;;
      esac
    done
  }

  # Setup timeout handler
  handle_timeout() {
    echo "Operation timed out" >&2
    kill "$COPROC_PID" 2>/dev/null
    exit 1
  }

  trap 'handle_timeout' SIGALRM

  # Interactive loop
  while IFS= read -r -p "> " cmd; do
    # Send command
    echo "$cmd" >&"${COPROC[1]}"

    # Wait for response with timeout
    read -t "$timeout" -r response <&"${COPROC[0]}" || {
      echo "No response received" >&2
      continue
    }

    echo "Response: $response"
  done

  # Cleanup
  echo "exit" >&"${COPROC[1]}"
  wait "$COPROC_PID" 2>/dev/null
}

# Usage
interactive_coprocess

This comprehensive guide covers various advanced redirection patterns and techniques in Linux, including stdout/stderr separation, pipeline error handling, and bidirectional pipes with coprocess communication. These examples demonstrate how to effectively manage and handle data streams and errors in Linux shell scripting.








































Performance Optimization Guide
1. Buffer Management
Buffer Size Analysis

This script analyzes the optimal buffer size for a given file by testing various buffer sizes and measuring the performance.
bash

#!/bin/bash
# Analyze optimal buffer size
analyze_buffer_size() {
  local file=$1
  local sizes=(4096 8192 16384 32768 65536 131072)

  for size in "${sizes[@]}"; do
    echo "Testing buffer size: $size bytes"
    # Time the operation
    time (
      dd if="$file" bs=$size 2>/dev/null |
      dd of=/dev/null 2>/dev/null
    )
  done
}

# Get current pipe buffer size
get_pipe_buffer() {
  local pipe=$(mktemp -u)
  mkfifo "$pipe"
  local size=$(fcntl -g "$pipe" F_GETPIPE_SZ)
  rm "$pipe"
  echo "Current pipe buffer size: $size bytes"
}

Memory-Aware Buffering

This script dynamically adjusts the buffer size for I/O operations based on the available system memory.
bash

#!/bin/bash
# Adaptive buffer sizing based on system memory
get_optimal_buffer() {
  local available_mem=$(free -b | awk '/Mem:/ {print $7}')
  local buffer_size=$((available_mem / 100)) # Use 1% of available memory

  # Cap at max pipe size
  local max_pipe_size=$(cat /proc/sys/fs/pipe-max-size 2>/dev/null || echo 65536)
  if ((buffer_size > max_pipe_size)); then
    buffer_size=$max_pipe_size
  fi

  echo $buffer_size
}

# Usage in I/O operation
buffer_size=$(get_optimal_buffer)
dd bs=$buffer_size if="$input" of="$output"

2. Pipeline Optimization
Pipeline Stage Analysis

This script analyzes the performance of each stage in a pipeline, including resource usage monitoring.
bash

#!/bin/bash
# Analyze pipeline performance
analyze_pipeline() {
  local cmds=("$@")
  local -a pids
  local start_time=$(date +%s%N)

  # Resource monitoring
  monitor_resources() {
    local pid=$1
    while kill -0 $pid 2>/dev/null; do
      ps -o pid,ppid,%cpu,%mem,rss,cmd -p $pid
      sleep 1
    done
  }

  # Execute pipeline with monitoring
  {
    for cmd in "${cmds[@]}"; do
      eval "$cmd" &
      pids+=($!)
      monitor_resources $! &
    done | ts '[%Y-%m-%d %H:%M:%S]'
  } > pipeline_stats.log
  wait "${pids[@]}"
  local end_time=$(date +%s%N)
  echo "Total time: $(((end_time - start_time)/1000000))ms"
}

1 vulnerability detected

I/O Bottleneck Detection

This script monitors I/O performance for a given process, providing detailed statistics.
bash

#!/bin/bash
# Monitor I/O performance
monitor_io() {
  local pid=$1
  local interval=${2:-1}

  while kill -0 $pid 2>/dev/null; do
    # I/O stats
    local io_stats=$(cat /proc/$pid/io 2>/dev/null)

    # Disk stats
    local disk_stats=$(iostat -x 1 1)

    # Process stats
    local process_stats=$(ps -o pid,ppid,%cpu,%mem,rss -p $pid)

    echo "=== $(date) ==="
    echo "I/O Stats:"
    echo "$io_stats"
    echo "Disk Stats:"
    echo "$disk_stats"
    echo "Process Stats:"
    echo "$process_stats"

    sleep $interval
  done
}

3. System Limits Management
Resource Limit Monitoring

This script provides a comprehensive overview of system resource limits.
bash

#!/bin/bash
# Comprehensive resource monitoring
check_system_limits() {
  echo "=== System Limits ==="

  # File descriptor limits
  echo "File Descriptors:"
  echo " Soft limit: $(ulimit -Sn)"
  echo " Hard limit: $(ulimit -Hn)"

  # Process limits
  echo "Process Limits:"
  echo " Max processes: $(ulimit -u)"

  # Memory limits
  echo "Memory Limits:"
  echo " Max memory size: $(ulimit -m) KB"
  echo " Virtual memory: $(ulimit -v) KB"

  # Open files
  echo "Current Open Files:"
  lsof | wc -l

  # Available memory
  echo "Memory Status:"
  free -h
}

Resource Exhaustion Prevention

This script proactively manages resources to prevent exhaustion by monitoring and controlling memory and CPU usage.
bash

#!/bin/bash
# Proactive resource management
manage_resources() {
  local pid=$1
  local max_mem=$2 # MB
  local max_cpu=$3 # Percentage

  while kill -0 $pid 2>/dev/null; do
    # Check memory usage
    local mem_usage=$(ps -o rss= -p $pid)
    mem_usage=$((mem_usage/1024)) # Convert to MB

    # Check CPU usage
    local cpu_usage=$(ps -o %cpu= -p $pid)

    if ((mem_usage > max_mem)); then
      echo "Memory limit exceeded ($mem_usage MB)" >&2
      kill -15 $pid
      break
    fi

    if ((${cpu_usage%.*} > max_cpu)); then
      echo "CPU limit exceeded ($cpu_usage%)" >&2
      kill -15 $pid
      break
    fi

    sleep 1
  done
}

# Usage
process_with_limits() {
  local cmd=$1
  eval "$cmd" &
  local pid=$!
  manage_resources $pid 1000 90 # 1GB RAM, 90% CPU
  wait $pid
}

Dynamic Resource Adaptation

This script dynamically adjusts resource limits for a process to ensure successful execution within specified constraints.
bash

#!/bin/bash
# Adaptive resource management
adapt_resources() {
  local cmd=$1
  local min_mem=100 # MB
  local max_mem=1000 # MB
  local mem_step=100 # MB

  local current_mem=$min_mem
  local success=false

  while ((current_mem <= max_mem)) && ! $success; do
    echo "Attempting with ${current_mem}MB memory limit..."
    # Set resource limits
    ulimit -v $((current_mem * 1024))
    if eval "$cmd"; then
      success=true
      echo "Succeeded with ${current_mem}MB limit"
    else
      echo "Failed with ${current_mem}MB limit"
      current_mem=$((current_mem + mem_step))
    fi
  done

  if ! $success; then
    echo "Failed to execute within resource limits" >&2
    return 1
  fi
}

This guide provides various scripts and techniques for optimizing I/O performance in Linux, including buffer management, pipeline optimization, and system limits management. These scripts help analyze and adapt resource usage to improve overall system efficiency.





















Linux I/O Performance Optimization Guide
1. Buffer Management
Understanding Buffer Sizes

This script displays the current system buffer settings, including pipe buffer size, read/write buffer sizes, and system page size.
bash

#!/bin/bash
# Display current system buffer settings
show_buffer_settings() {
  # Pipe buffer size
  echo "Default pipe buffer size:"
  cat /proc/sys/fs/pipe-max-size

  # Read buffer
  echo -e "\nDefault read buffer (min/default/max):"
  sysctl net.ipv4.tcp_rmem

  # Write buffer
  echo -e "\nDefault write buffer (min/default/max):"
  sysctl net.ipv4.tcp_wmem

  # Page size
  echo -e "\nSystem page size:"
  getconf PAGE_SIZE
}

Buffer Size Impact Analysis

This script tests different buffer sizes to analyze their impact on performance for a given input and output file. It also monitors memory pressure.
bash

#!/bin/bash
# Test different buffer sizes
analyze_buffer_performance() {
  local input_file=$1
  local output_file=$2

  # Test various buffer sizes
  for size in 512 1024 4096 8192 16384 32768 65536 131072; do
    echo "Testing buffer size: $size bytes"
    # Clear cache
    echo 3 > /proc/sys/vm/drop_caches

    time dd if="$input_file" of="$output_file" bs=$size 2>&1 | grep "bytes/sec"

    # Monitor memory pressure
    free -m
    vmstat 1 5
  done
}

Context Switch Analysis

This script monitors context switching overhead for a given process ID (PID) over a specified duration.
bash

#!/bin/bash
# Monitor context switching overhead
monitor_context_switches() {
  local pid=$1
  local duration=$2

  echo "Starting context switch monitoring for PID $pid"

  # Record initial values
  local start_voluntary=$(grep "voluntary_ctxt_switches" /proc/$pid/status | awk '{print $2}')
  local start_nonvoluntary=$(grep "nonvoluntary_ctxt_switches" /proc/$pid/status | awk '{print $2}')

  sleep "$duration"

  # Record end values
  local end_voluntary=$(grep "voluntary_ctxt_switches" /proc/$pid/status | awk '{print $2}')
  local end_nonvoluntary=$(grep "nonvoluntary_ctxt_switches" /proc/$pid/status | awk '{print $2}')

  # Calculate rates
  echo "Context switches per second:"
  echo "Voluntary: $(((end_voluntary - start_voluntary) / duration))"
  echo "Non-voluntary: $(((end_nonvoluntary - start_nonvoluntary) / duration))"
}

2. Pipeline Optimization
Pipeline Stage Analysis

This script analyzes the performance of each stage in a pipeline, including resource usage monitoring.
bash

#!/bin/bash
# Analyze pipeline performance
analyze_pipeline() {
  local cmd="$1"
  local stages=${2:-1}

  # Monitor resources for each stage
  for ((i=1; i<=stages; i++)); do
    echo "=== Stage $i ==="

    # Start command with resource monitoring
    $cmd | \
    while IFS= read -r line; do
      # Process monitoring
      ps -o pid,ppid,%cpu,%mem,vsz,rss,cmd -p $$

      # I/O statistics
      iostat -x 1 1

      echo "$line"
    done
  done
}

Memory Usage Monitoring

This script provides comprehensive memory monitoring for a given process ID (PID) at regular intervals.
bash

#!/bin/bash
# Comprehensive memory monitoring
monitor_memory() {
  local pid=$1
  local interval=${2:-1}

  while kill -0 $pid 2>/dev/null; do
    echo "=== $(date) ==="

    # Process memory
    ps -o pid,ppid,%mem,vsz,rss -p $pid

    # System memory
    free -m

    # Memory pressure
    grep -E "^(Active|Inactive|Dirty|Writeback):" /proc/meminfo

    sleep "$interval"
  done
}

I/O Bottleneck Detection

This script detects I/O bottlenecks by monitoring various I/O performance metrics over a specified duration.
bash

#!/bin/bash
# I/O performance analysis
detect_io_bottlenecks() {
  local duration=${1:-60}
  local interval=${2:-1}

  echo "Starting I/O bottleneck detection for ${duration}s"

  # Start monitoring
  {
    # I/O wait time
    iostat -x "$interval" "$duration"
    # Disk utilization
    iotop -b -n "$((duration/interval))"
    # Block device statistics
    while ((duration > 0)); do
      echo "=== $(date) ==="
      cat /proc/diskstats
      sleep "$interval"
      ((duration-=interval))
    done
  } | grep -E "await|svctm|^Device|^[hsv]d[a-z]"
}

3. System Limits Management
Resource Limits Analysis

This script provides a comprehensive overview of system resource limits, including file descriptors, process limits, and memory limits.
bash

#!/bin/bash
# Comprehensive system limits check
check_system_limits() {
  echo "=== System Resource Limits ==="

  # File descriptor limits
  echo "File descriptor limits:"
  ulimit -n
  cat /proc/sys/fs/file-max

  # Process limits
  echo -e "\nProcess limits:"
  ulimit -u
  cat /proc/sys/kernel/pid_max

  # Memory limits
  echo -e "\nMemory limits:"
  ulimit -m
  ulimit -v
  cat /proc/sys/vm/max_map_count

  # Open files
  echo -e "\nCurrent open files:"
  lsof | wc -l

  # Resource usage
  echo -e "\nResource usage:"
  cat /proc/self/limits
}

Resource Exhaustion Prevention

This script proactively monitors system resources to prevent exhaustion by issuing alerts and managing usage.
bash

#!/bin/bash
# Proactive resource monitoring
monitor_resources() {
  local warning_threshold=80 # Percentage
  local critical_threshold=90 # Percentage

  while true; do
    # CPU usage
    local cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}')

    # Memory usage
    local mem_usage=$(free | grep Mem | awk '{print $3/$2 * 100}')

    # File descriptor usage
    local fd_usage=$(lsof | wc -l)
    local fd_max=$(cat /proc/sys/fs/file-max)
    local fd_percent=$((fd_usage * 100 / fd_max))

    # Process count
    local process_count=$(ps aux | wc -l)
    local process_max=$(ulimit -u)
    local process_percent=$((process_count * 100 / process_max))

    # Check thresholds and alert
    for metric in "CPU:$cpu_usage" "Memory:$mem_usage" "FD:$fd_percent" "Process:$process_percent"; do
      local name=${metric%:*}
      local value=${metric#*:}
      if (( $(echo "$value > $critical_threshold" | bc -l) )); then
        echo "CRITICAL: $name usage at ${value}%"
      elif (( $(echo "$value > $warning_threshold" | bc -l) )); then
        echo "WARNING: $name usage at ${value}%"
      fi
    done
    sleep 5
  done
}

Adaptive Resource Management

This script dynamically adjusts resource limits for a process to ensure successful execution within specified constraints.
bash

#!/bin/bash
# Dynamic resource adjustment
adjust_resources() {
  local pid=$1
  local max_mem=${2:-80} # Max memory percentage
  local max_cpu=${3:-80} # Max CPU percentage

  while kill -0 $pid 2>/dev/null; do
    # Get current usage
    local mem_usage=$(ps -o %mem= -p $pid)
    local cpu_usage=$(ps -o %cpu= -p $pid)

    # Check memory
    if (( $(echo "$mem_usage > $max_mem" | bc -l) )); then
      echo "Memory threshold exceeded ($mem_usage%)"
      renice +1 $pid
    fi

    # Check CPU
    if (( $(echo "$cpu_usage > $max_cpu" | bc -l) )); then
      echo "CPU threshold exceeded ($cpu_usage%)"
      renice +1 $pid
    fi

    sleep 1
  done
}

# Usage example
my_command &
adjust_resources $! 80 80

This guide provides various scripts and techniques for optimizing I/O performance in Linux, including buffer management, pipeline optimization, and system limits management. These scripts help analyze and adapt resource usage to improve overall system efficiency.




















Security Guide
1. File Permission Management
Permission Verification

This script verifies file permissions, ownership, and directory hierarchy permissions.
bash

#!/bin/bash
# Comprehensive permission checking
verify_permissions() {
  local target=$1
  local required_perms=$2
  local user=$3

  # Check if file exists
  [[ -e "$target" ]] || {
    echo "Target does not exist: $target" >&2
    return 1
  }

  # Get actual permissions
  local actual_perms=$(stat -c '%a' "$target")

  # Check ownership
  local owner=$(stat -c '%U' "$target")
  local group=$(stat -c '%G' "$target")

  # Verify directory hierarchy permissions
  local dir=$target
  while [[ "$dir" != "/" ]]; do
    dir=$(dirname "$dir")
    if ! [[ -x "$dir" ]]; then
      echo "Missing execute permission on: $dir" >&2
      return 1
    fi
  done

  # Check symbolic links
  if [[ -L "$target" ]]; then
    local real_path=$(readlink -f "$target")
    verify_permissions "$real_path" "$required_perms" "$user"
    return $?
  fi

  # Verify permissions match requirements
  [[ "$actual_perms" == "$required_perms" ]]
}

# Usage example:
# verify_permissions "/path/to/file" "755" "username"

Secure File Creation

This script creates files and directories securely, ensuring proper permissions and atomic operations.
bash

#!/bin/bash
# Create files securely
secure_create() {
  local path=$1
  local mode=${2:-0600}

  # Create with restrictive umask
  (
    umask 077
    if ! : > "$path.tmp.$$"; then
      echo "Failed to create temporary file" >&2
      return 1
    fi
  )

  # Set requested permissions
  chmod "$mode" "$path.tmp.$$"

  # Atomic rename
  mv "$path.tmp.$$" "$path"
}

# Directory creation with proper permissions
secure_mkdir() {
  local dir=$1
  local mode=${2:-0700}

  # Create parent directories securely
  local parent=$(dirname "$dir")
  [[ -d "$parent" ]] || secure_mkdir "$parent" "$mode"

  # Create directory with temporary restrictive permissions
  mkdir -m 0700 "$dir"

  # Set final permissions
  chmod "$mode" "$dir"
}

# Usage examples:
# secure_create "/path/to/file" "0644"
# secure_mkdir "/path/to/directory" "0755"

2. Secure Practices
Privilege Management

This script demonstrates secure privilege handling, including dropping and temporarily elevating privileges.
bash

#!/bin/bash
# Secure privilege handling
drop_privileges() {
  local user=$1
  local group=${2:-$user}

  # Verify user exists
  id "$user" &>/dev/null || {
    echo "User does not exist: $user" >&2
    return 1
  }

  # Drop supplementary groups
  if ! groups="$(id -G)"; then
    echo "Failed to get groups" >&2
    return 1
  fi

  # Switch to target user/group
  if ! sudo -u "$user" -g "$group" "$@"; then
    echo "Failed to drop privileges" >&2
    return 1
  fi
}

# Temporary privilege elevation
with_elevated_privs() {
  local cmd=$1
  local timestamp=/tmp/.sudo_timestamp.$$

  # Create sudo timestamp
  touch "$timestamp"
  chmod 0600 "$timestamp"

  # Execute with elevated privileges
  SUDO_ASKPASS=/dev/null sudo -A "$cmd"
  local status=$?

  # Clean up
  shred -u "$timestamp"
  return $status
}

# Usage examples:
# drop_privileges "username" "groupname" "command"
# with_elevated_privs "command"

Resource Limitation

This script enforces resource limits and process isolation.
bash

#!/bin/bash
# Secure resource limiting
enforce_limits() {
  local cmd=$1
  shift

  # Set resource limits
  ulimit -n 1024 # File descriptors
  ulimit -u 100 # Processes
  ulimit -f 100000 # File size (blocks)
  ulimit -v 500000 # Virtual memory (KB)

  # Clear environment
  env -i \
  HOME="$HOME" \
  PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin" \
  "$cmd" "$@"
}

# Process isolation
isolate_process() {
  local cmd=$1
  shift

  # Create new namespace
  unshare --mount --uts --ipc --net --pid --fork \
  chroot /path/to/jail \
  "$cmd" "$@"
}

# Usage examples:
# enforce_limits "command"
# isolate_process "command"

Input Validation

This script securely handles and validates input.
bash

#!/bin/bash
# Secure input handling
validate_input() {
  local input=$1
  local pattern=$2

  # Remove null bytes
  input=${input//$'\0'/}

  # Basic sanitization
  input=$(printf '%s' "$input" | sed 's/[^[:print:]]//g')

  # Pattern matching
  [[ "$input" =~ $pattern ]] || {
    echo "Invalid input format" >&2
    return 1
  }

  echo "$input"
}

# Path validation
validate_path() {
  local path=$1

  # Normalize path
  path=$(realpath -m "$path")

  # Check for directory traversal
  case "$path" in
    */../*|*/./*)
      echo "Invalid path: contains directory traversal" >&2
      return 1
      ;;
  esac

  echo "$path"
}

# Usage examples:
# validate_input "user_input" "^[a-zA-Z0-9_-]+$"
# validate_path "/path/to/validate"

3. Data Security
Secure Data Handling

This script ensures secure data handling through secure pipes and memory management.
bash

#!/bin/bash
# Secure pipe data handling
secure_pipe() {
  local cmd=$1
  local tmpdir=$(mktemp -d)

  # Set restrictive permissions
  chmod 700 "$tmpdir"

  # Create named pipe
  mkfifo -m 600 "$tmpdir/pipe"

  # Execute command with secure pipe
  (
    exec 3<>"$tmpdir/pipe"
    rm -f "$tmpdir/pipe"
    rmdir "$tmpdir"
    eval "$cmd" <&3
    exec 3>&-
  )
}

# Secure memory handling
secure_memory() {
  local cmd=$1

  # Lock memory
  mlockall MCL_CURRENT MCL_FUTURE

  # Clear sensitive environment variables
  unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY MYSQL_PWD

  # Execute command
  "$cmd"

  # Clear memory on exit
  mlockall MCL_RESET
}

# Usage examples:
# secure_pipe "command"
# secure_memory "command"

Secure Cleanup

This script demonstrates secure data cleanup and environment sanitization.
bash

#!/bin/bash
# Secure data cleanup
secure_cleanup() {
  local file=$1
  local passes=${2:-3}

  # Verify file exists
  [[ -f "$file" ]] || {
    echo "File not found: $file" >&2
    return 1
  }

  # Secure overwrite
  for ((i=1; i<=passes; i++)); do
    dd if=/dev/urandom of="$file" bs=8192 conv=notrunc
    sync "$file"
  done

  # Remove file
  rm -f "$file"
}

# Environment sanitization
sanitize_env() {
  # Clear sensitive variables
  unset $(env | grep -i 'key\|pass\|secret' | cut -d= -f1)
  # Set safe PATH
  PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
  # Clear other potentially dangerous variables
  unset LD_LIBRARY_PATH LD_PRELOAD PYTHONPATH PERL5LIB
}

# Usage examples:
# secure_cleanup "/path/to/file"
# sanitize_env

Error Message Security

This script ensures secure error handling by sanitizing error messages and logging them securely.
bash

#!/bin/bash
# Secure error handling
secure_error() {
  local error_msg=$1
  local log_file=$2
  local severity=${3:-ERROR}

  # Sanitize error message
  error_msg=$(printf '%s' "$error_msg" | sed 's/[^[:print:]]//g')

  # Remove sensitive information
  error_msg=$(echo "$error_msg" | sed -E 's/pass(word)?=.[^&]*&/password=REDACTED&/g')

  # Log error securely
  logger -p "user.$severity" "$error_msg"

  if [[ -n "$log_file" ]]; then
    printf '[%s] %s: %s\n' "$(date -u +%Y-%m-%dT%H:%M:%SZ)" "$severity" "$error_msg" >> "$log_file"
  fi
}

# Usage example:
# secure_error "An error occurred: password=secret123&" "/path/to/logfile.log"

This comprehensive guide covers various security practices in Unix/Linux, including file permission management, secure practices, data security, and error message security. These scripts help ensure a secure environment by managing permissions, handling privileges, validating input, and securely managing data and error messages.




















Best Practices Guide
1. Error Handling Best Practices
Comprehensive Error Management

This script enables strict error handling and sets up a global error handler to manage errors and clean up resources.
bash

#!/bin/bash
# Enable strict error handling
set -euo pipefail
IFS=$'\n\t'

# Global error handler
trap 'error_handler $? $LINENO $BASH_LINENO "$BASH_COMMAND" $(printf "::%s" ${FUNCNAME[@]:-})' ERR

error_handler() {
  local exit_code=$1
  local line_no=$2
  local bash_lineno=$3
  local last_cmd=$4
  local func_trace=$5

  echo "Error on line ${line_no}, command: ${last_cmd}" >&2
  echo "Function trace: ${func_trace}" >&2

  # Cleanup resources
  cleanup_handler

  exit "${exit_code}"
}

# Resource cleanup
cleanup_handler() {
  local tmpfiles=(/tmp/script.*.tmp)
  local tmpdirs=(/tmp/script.*.tmp.d)

  # Remove temporary files
  for file in "${tmpfiles[@]}"; do
    [[ -f "$file" ]] && rm -f "$file"
  done

  # Remove temporary directories
  for dir in "${tmpdirs[@]}"; do
    [[ -d "$dir" ]] && rm -rf "$dir"
  done
}

# Set cleanup trap
trap cleanup_handler EXIT

# Example usage
# your_script_content_here

Pipeline Error Management

This script handles errors in a pipeline, logging them to a specified file.
bash

#!/bin/bash
# Pipeline with error handling
pipeline_with_errors() {
  local input=$1
  local output=$2
  local error_log=$3

  # Create named pipes for error handling
  local err_pipe=$(mktemp -u)
  mkfifo "$err_pipe"

  # Start error collector
  {
    while IFS= read -r line; do
      echo "[$(date '+%Y-%m-%d %H:%M:%S')] $line" >> "$error_log"
    done < "$err_pipe"
  } &
  local collector_pid=$!

  # Execute pipeline with error redirection
  {
    command1 < "$input" 2>"$err_pipe" |
    command2 2>"$err_pipe" |
    command3 > "$output" 2>"$err_pipe"
  }
  local pipeline_status=${PIPESTATUS[*]}

  # Cleanup
  exec {err_pipe}>&-
  rm -f "$err_pipe"
  wait "$collector_pid"

  # Check pipeline status
  for status in $pipeline_status; do
    if ((status != 0)); then
      echo "Pipeline failed with status: $pipeline_status" >&2
      return 1
    fi
  done
}

# Example usage
# pipeline_with_errors "input_file" "output_file" "error_log"

2. File Operation Best Practices
Secure File Operations

This script demonstrates secure file handling, including validation, locking, and atomic updates.
bash

#!/bin/bash
# Secure file handling template
secure_file_ops() {
  local input=$1
  local output=$2

  # Validate paths
  local real_input=$(realpath -e "$input") || {
    echo "Invalid input path: $input" >&2
    return 1
  }

  local output_dir=$(dirname "$output")
  [[ -d "$output_dir" ]] || mkdir -p "$output_dir"

  # Create lock file
  local lock_file="${output}.lock"
  exec {lock_fd}>"$lock_file"
  flock -x -w 10 "$lock_fd" || {
    echo "Could not acquire lock" >&2
    return 1
  }

  # Create temporary file
  local temp_file="${output}.tmp.$$"

  # Ensure cleanup
  trap 'rm -f "$temp_file" "$lock_file"' EXIT

  # Process file atomically
  if process_file "$real_input" > "$temp_file"; then
    mv "$temp_file" "$output"
  else
    echo "Processing failed" >&2
    return 1
  fi
}

# Example usage
# secure_file_ops "input_file" "output_file"

Atomic File Updates

This script demonstrates atomic file updates, ensuring that file writes are atomic and secure.
bash

#!/bin/bash
# Atomic file update pattern
atomic_update() {
  local file=$1
  local content=$2

  # Create temporary file with secure permissions
  local temp_file
  temp_file=$(mktemp "${file}.XXXXXX") || {
    echo "Failed to create temporary file" >&2
    return 1
  }

  # Set restrictive permissions
  chmod 600 "$temp_file"

  # Write content
  printf '%s\n' "$content" > "$temp_file"

  # Atomic rename
  mv "$temp_file" "$file"
}

# Example usage
# atomic_update "file_path" "content"

1 vulnerability detected

3. Pipeline Design Best Practices
Pipeline Testing Framework

This script provides a framework for testing pipelines with different datasets and memory monitoring.
bash

#!/bin/bash
# Pipeline testing framework
test_pipeline() {
  local pipeline_cmd=$1
  local test_data=$2
  local expected_output=$3

  # Create test environment
  local test_dir=$(mktemp -d)
  trap 'rm -rf "$test_dir"' EXIT

  # Generate test data
  if [[ -n "$test_data" ]]; then
    echo "$test_data" > "$test_dir/input"
  else
    head -c 1M /dev/urandom > "$test_dir/input"
  fi

  # Test small dataset
  echo "Testing with small dataset..."
  head -c 1K "$test_dir/input" | eval "$pipeline_cmd" > "$test_dir/small_output"

  # Test full dataset with memory monitoring
  echo "Testing with full dataset..."
  {
    eval "$pipeline_cmd" < "$test_dir/input" > "$test_dir/output"
  } 2> >(
    while IFS= read -r line; do
      echo "[Memory] $line" >&2
    done < <(free -m)
  )

  # Verify output
  if [[ -n "$expected_output" ]]; then
    diff "$test_dir/output" "$expected_output"
  fi
}

# Example usage
# test_pipeline "pipeline_command" "test_data" "expected_output"

Pipeline Documentation Generator

This script generates documentation for a pipeline, detailing its structure, stages, and error handling.
bash

#!/bin/bash
# Generate pipeline documentation
document_pipeline() {
  local pipeline_file=$1
  local doc_file=$2

  {
    echo "# Pipeline Documentation"
    echo "Generated: $(date)"
    echo
    echo "## Pipeline Structure"
    # Extract and document pipeline stages
    grep -E '\|' "$pipeline_file" | while IFS= read -r line; do
      echo "### Stage: $line"
      echo '```bash'
      echo "$line"
      echo '```'
      echo
      # Extract commands
      echo "#### Commands:"
      echo "$line" | tr '|' '\n' | while IFS= read -r cmd; do
        echo "- \`$cmd\`"
        # Document command purpose if available
        man "$cmd" 2>/dev/null | head -n 5
      done
      echo
    done
    echo "## Error Handling"
    grep -E 'trap|ERR|exit' "$pipeline_file"
    echo "## Resource Usage"
    grep -E 'ulimit|nice|renice' "$pipeline_file"
  } > "$doc_file"
}

# Example usage
# document_pipeline "pipeline_file.sh" "documentation.md"

1 vulnerability detected

4. General Guidelines Implementation
Process Substitution Pattern

This script demonstrates advanced process substitution usage for logging and monitoring command execution.
bash

#!/bin/bash
# Advanced process substitution usage
process_with_logging() {
  local cmd=$1
  local log_dir=$2

  # Ensure log directory exists
  mkdir -p "$log_dir"

  # Execute with logging
  eval "$cmd" \
    > >(tee "$log_dir/stdout.log") \
    2> >(tee "$log_dir/stderr.log" >&2) \
    &

  local pid=$!

  # Monitor execution
  while kill -0 $pid 2>/dev/null; do
    ps -o pid,ppid,%cpu,%mem,cmd -p $pid >> "$log_dir/stats.log"
    sleep 1
  done

  wait $pid
}

# Example usage
# process_with_logging "command" "/path/to/log_dir"

1 vulnerability detected

Named Pipe Utilities

This script provides utilities for managing named pipes, including creation and cleanup.
bash

#!/bin/bash
# Named pipe management utilities
create_pipe_network() {
  local base_dir=$1
  local pipe_count=$2

  # Create pipe directory
  mkdir -p "$base_dir"

  # Create numbered pipes
  for ((i=1; i<=pipe_count; i++)); do
    mkfifo "$base_dir/pipe$i"
  done

  # Create cleanup trap
  trap 'rm -rf "$base_dir"' EXIT

  # Start readers
  for ((i=1; i<=pipe_count; i++)); do
    {
      while IFS= read -r line; do
        process_data "$line" "$i"
      done < "$base_dir/pipe$i"
    } &
  done
}

# Example usage
# create_pipe_network "/path/to/pipes" 5

Code Readability Pattern

This script provides a template for readable pipeline code, organizing the pipeline into stages and ensuring proper setup and cleanup.
bash

#!/bin/bash
# Template for readable pipeline code
main() {
  # Configuration
  local input=${1:?Missing input file}
  local output=${2:?Missing output file}
  local log_dir=${3:-/var/log/pipeline}

  # Initialization
  setup_environment() {
    mkdir -p "$log_dir"
    trap cleanup EXIT
  }

  # Pipeline stages
  stage1() {
    preprocess_data < "$input"
  }

  stage2() {
    transform_data
  }

  stage3() {
    postprocess_data > "$output"
  }

  # Main pipeline
  setup_environment

  stage1 |
  stage2 |
  stage3 |
  tee >(log_statistics > "$log_dir/stats.log")
}

# Execute main function
main "$@"

This guide provides best practices for error handling, secure file operations, pipeline design, and general guidelines for Linux I/O operations. These scripts help ensure robust, secure, and efficient handling of I/O tasks.






























Debugging and Troubleshooting Guide
1. Common Issues Resolution
Redirection Debugging

This script helps debug redirection issues by logging command execution details and analyzing file descriptors.
bash

#!/bin/bash
# Debug redirection issues
debug_redirections() {
  local cmd=$1
  local debug_log=${2:-/tmp/redirect_debug.log}

  # Enable debugging
  set -x

  # Create debug log
  exec 5>"$debug_log"
  BASH_XTRACEFD=5

  # Test redirections
  {
    eval "$cmd"
  } > >(tee stdout.log) 2> >(tee stderr.log >&2)

  # Check file descriptors
  echo "=== File Descriptors ===" >&5
  ls -l /proc/$$/fd >&5

  # Analyze redirection order
  echo "=== Command Analysis ===" >&5
  bash -x -c "$cmd" 2>&5

  # Disable debugging
  set +x
  exec 5>&-
}

Permission Analysis

This script analyzes permission issues for a given path, including checking the directory hierarchy and effective permissions.
bash

#!/bin/bash
# Analyze permission issues
check_permissions() {
  local path=$1
  local required_perms=$2

  echo "=== Permission Analysis ==="

  # Check path hierarchy
  while [[ "$path" != "/" ]]; do
    echo "Checking: $path"
    ls -ld "$path"
    stat -c "Access: %a Owner: %U Group: %G" "$path"

    # Check effective permissions
    sudo -u "$USER" test -r "$path" && echo "Readable: Yes" || echo "Readable: No"
    sudo -u "$USER" test -w "$path" && echo "Writable: Yes" || echo "Writable: No"
    sudo -u "$USER" test -x "$path" && echo "Executable: Yes" || echo "Executable: No"

    path=$(dirname "$path")
    echo
  done
}

Deadlock Detection

This script detects deadlocks in a pipeline by analyzing the state and file descriptors of given PIDs.
bash

#!/bin/bash
# Detect pipeline deadlocks
detect_deadlocks() {
  local pids=("$@")
  echo "=== Deadlock Analysis ==="
  for pid in "${pids[@]}"; do
    echo "Analyzing PID: $pid"
    # Check process state
    ps -o pid,ppid,stat,wchan -p "$pid"
    # Check file descriptors
    echo "File descriptors:"
    ls -l "/proc/$pid/fd"
    # Check for blocked I/O
    echo "Blocked I/O:"
    cat "/proc/$pid/wchan"
    # Stack trace
    echo "Stack trace:"
    sudo gdb -p "$pid" -batch -ex "thread apply all bt"
  done
}

2. Debugging Techniques
System Call Tracing

This script uses strace to trace system calls, providing detailed timing and analysis of the results.
bash

#!/bin/bash
# Advanced strace wrapper
trace_syscalls() {
  local cmd=$1
  local output=${2:-/tmp/strace.log}

  # Trace with detailed timing
  strace -tt -f -o "$output" \
    -e trace=file,process,network,signal,ipc \
    bash -c "$cmd"

  # Analyze results
  echo "=== System Call Analysis ==="
  echo "Top syscalls:"
  grep -v '\\n' "$output" | cut -d'(' -f1 | sort | uniq -c | sort -rn | head -10

  echo -e "\nFile operations:"
  grep -E 'open|read|write|close' "$output" | cut -d'(' -f1 | sort | uniq -c

  echo -e "\nErrors:"
  grep 'ENOENT\|EPERM\|EACCES' "$output"
}

Resource Monitoring

This script monitors resources for a given PID, logging various metrics at regular intervals.
bash

#!/bin/bash
# Comprehensive resource monitoring
monitor_resources() {
  local pid=$1
  local interval=${2:-1}
  local output=${3:-/tmp/resource_monitor.log}

  {
    echo "=== Resource Monitoring Started ==="
    date

    while kill -0 "$pid" 2>/dev/null; do
      echo "=== $(date) ==="

      # Process stats
      ps -o pid,ppid,%cpu,%mem,vsz,rss,stat,wchan -p "$pid"

      # File descriptors
      echo "Open files:"
      lsof -p "$pid"

      # Memory map
      echo "Memory map:"
      pmap -x "$pid"

      # I/O stats
      echo "I/O statistics:"
      cat "/proc/$pid/io"
      sleep "$interval"
    done
  } > "$output"
}

Pipeline Analysis

This script analyzes each stage of a pipeline, monitoring resources and logging output and errors.
bash

#!/bin/bash
# Pipeline stage analysis
analyze_pipeline() {
  local cmds=("$@")
  local -a pids
  local debug_dir=$(mktemp -d)

  echo "=== Pipeline Analysis ==="

  # Start pipeline with monitoring
  for cmd in "${cmds[@]}"; do
    {
      bash -x -c "$cmd"
    } > "$debug_dir/stage_$$.out" 2> "$debug_dir/stage_$$.err" &
    pids+=($!)
  done

  # Monitor each stage
  for pid in "${pids[@]}"; do
    monitor_resources "$pid" 1 "$debug_dir/resources_$pid.log" &
  done

  # Wait for completion
  wait "${pids[@]}"

  # Analyze results
  for pid in "${pids[@]}"; do
    echo "Stage PID $pid analysis:"
    awk '/===/ {p=1; next} p {print}' "$debug_dir/resources_$pid.log"
  done

  rm -rf "$debug_dir"
}

3. Diagnostic Tools Integration
System Activity Reporter

This script uses sar to monitor system activity over a specified duration, collecting various metrics.
bash

#!/bin/bash
# Enhanced system activity monitoring
monitor_system_activity() {
  local duration=$1
  local interval=${2:-1}
  local output_dir=${3:-/tmp/sar_reports}

  mkdir -p "$output_dir"

  # Start SAR collection
  sar -o "$output_dir/sar.data" "$interval" "$duration" >/dev/null 2>&1 &
  local sar_pid=$!

  # Collect additional metrics
  while ((duration > 0)); do
    echo "=== $(date) ===" >> "$output_dir/detailed.log"
    # CPU info
    mpstat 1 1 >> "$output_dir/detailed.log"
    # Memory info
    vmstat 1 1 >> "$output_dir/detailed.log"
    # I/O stats
    iostat -x 1 1 >> "$output_dir/detailed.log"
    # Network stats
    netstat -s >> "$output_dir/detailed.log"
    ((duration--))
    sleep "$interval"
  done

  # Generate report
  {
    echo "=== System Activity Report ==="
    echo "Generated: $(date)"
    echo
    echo "=== CPU Usage ==="
    sar -u -f "$output_dir/sar.data"
    echo "=== Memory Usage ==="
    sar -r -f "$output_dir/sar.data"
    echo "=== I/O Activity ==="
    sar -b -f "$output_dir/sar.data"
    echo "=== Network Activity ==="
    sar -n DEV -f "$output_dir/sar.data"
  } > "$output_dir/report.txt"
  kill "$sar_pid"
}





Process Monitoring Integration

This script integrates various process monitoring tools to provide comprehensive analysis of a given PID.
bash

#!/bin/bash
# Integrated process monitoring
monitor_process() {
  local pid=$1
  local output_dir=${2:-/tmp/process_monitor}

  mkdir -p "$output_dir"

  # Start monitoring threads
  {
    while kill -0 "$pid" 2>/dev/null; do
      top -H -p "$pid" -b -n 1
      sleep 1
    done
  } > "$output_dir/threads.log" &

  # Monitor open files
  {
    while kill -0 "$pid" 2>/dev/null; do
      lsof -p "$pid"
      sleep 1
    done
  } > "$output_dir/files.log" &

  # Monitor network connections
  {
    while kill -0 "$pid" 2>/dev/null; do
      netstat -p | grep "$pid"
      sleep 1
    done
  } > "$output_dir/network.log" &

  # Wait for process to complete
  wait "$pid"

  # Generate report
  {
    echo "=== Process Analysis Report ==="
    echo "Process ID: $pid"
    echo "Runtime: $(ps -o etime= -p "$pid")"
    echo -e "\n=== Resource Usage Summary ==="
    awk '/CPU/ {cpu=$9} /MEM/ {mem=$10} END {print "CPU: "cpu"%, Memory: "mem"%"}' \
      "$output_dir/threads.log"
    echo -e "\n=== File Operations ==="
    sort "$output_dir/files.log" | uniq -c | sort -nr | head -10
    echo -e "\n=== Network Activity ==="
    sort "$output_dir/network.log" | uniq -c | sort -nr | head -10
  } > "$output_dir/report.txt"
}

This guide provides various scripts and techniques for debugging and troubleshooting, including error handling, resource monitoring, and integration with diagnostic tools. These scripts help diagnose and resolve common issues in Linux environments.


















Advanced Log Management
Robust Log Rotation System

This script manages logs with rotation, compression, and monitoring. It ensures logs do not consume excessive disk space and retains a specified number of log versions.
bash

#!/bin/bash
# Advanced log management with rotation, compression, and monitoring
manage_logs() {
  local log_file=$1
  local max_size=${2:-10485760} # 10MB default
  local keep_versions=${3:-5}
  local compress_threshold=${4:-1048576} # 1MB default

  # Create log directory structure
  local log_dir=$(dirname "$log_file")
  mkdir -p "$log_dir/archive"

  # Logging function with timestamps and severity
  log() {
    local severity=$1
    local message=$2
    local timestamp=$(date -u '+%Y-%m-%dT%H:%M:%S.%3NZ')

    # Format log entry
    printf '[%s] [%s] %s\n' \
      "$timestamp" \
      "$severity" \
      "$message" >> "$log_file"

    # Check size and rotate if needed
    if [[ -f "$log_file" ]] && (($(stat -c%s "$log_file") > max_size)); then
      rotate_logs
    fi
  }

  # Log rotation with compression
  rotate_logs() {
    local timestamp=$(date '+%Y%m%d_%H%M%S')
    local archive_base="${log_dir}/archive/$(basename "$log_file")"

    # Rotate existing archives
    for ((i=keep_versions; i>0; i--)); do
      local old_archive="${archive_base}.$((i-1)).gz"
      local new_archive="${archive_base}.$i.gz"
      [[ -f "$old_archive" ]] && mv "$old_archive" "$new_archive"
    done

    # Compress current log if larger than threshold
    if [[ -f "$log_file" ]] && (($(stat -c%s "$log_file") > compress_threshold)); then
      gzip -c "$log_file" > "${archive_base}.1.gz"
    else
      mv "$log_file" "${archive_base}.1"
    fi

    # Create new log file with proper permissions
    touch "$log_file"
    chmod 0640 "$log_file"
  }

  # Log analysis function
  analyze_logs() {
    echo "=== Log Analysis ==="
    echo "Current log size: $(stat -c%s "$log_file") bytes"
    echo "Archive contents:"
    ls -lh "${log_dir}/archive/"

    echo -e "\nError frequency:"
    grep -h '\[ERROR\]' "$log_file" "${log_dir}/archive/"* | \
      sort | \
      uniq -c | \
      sort -rn | \
      head -10
  }

  # Set up monitoring
  monitor_logs() {
    local check_interval=${1:-300} # 5 minutes default
    while true; do
      local current_size=$(stat -c%s "$log_file" 2>/dev/null || echo 0)
      local used_space=$(df -P "$log_dir" | awk 'NR==2 {print $5}' | tr -d '%')
      # Alert if log growing too fast
      if ((current_size > max_size * 2)); then
        log "WARNING" "Log file growing rapidly: $current_size bytes"
      fi
      # Alert if disk space low
      if ((used_space > 90)); then
        log "ERROR" "Disk space critical: ${used_space}% used"
        rotate_logs
      fi
      sleep "$check_interval"
    done
  }

  # Start log monitor in background
  monitor_logs &
  monitor_pid=$!

  # Cleanup on exit
  trap 'kill $monitor_pid 2>/dev/null' EXIT
  return 0
}

Log Processing and Analysis

This script processes logs in real-time, categorizing them by severity and generating statistics.
bash

#!/bin/bash
# Log processing with real-time monitoring and analysis
process_logs() {
  local input_log=$1
  local output_dir=$2
  local pattern_file=$3

  # Setup output directory
  mkdir -p "$output_dir"/{errors,warnings,critical,stats}

  # Real-time log processing
  tail -F "$input_log" | \
  while IFS= read -r line; do
    # Parse log entry
    local timestamp=$(echo "$line" | grep -oP '^\[\K[^\]]+')
    local severity=$(echo "$line" | grep -oP '\[\K(ERROR|WARNING|INFO|DEBUG|CRITICAL)[^\]]+')
    local message=${line#*]}

    # Route by severity
    case "$severity" in
      ERROR)
        echo "$line" >> "$output_dir/errors/error.log"
        notify_error "$message"
        ;;
      WARNING)
        echo "$line" >> "$output_dir/warnings/warning.log"
        ;;
      CRITICAL)
        echo "$line" >> "$output_dir/critical/critical.log"
        notify_critical "$message"
        ;;
    esac

    # Pattern matching
    if [[ -f "$pattern_file" ]]; then
      while IFS= read -r pattern; do
        if [[ "$message" =~ $pattern ]]; then
          echo "[MATCH] $line" >> "$output_dir/patterns.log"
        fi
      done < "$pattern_file"
    fi

    # Generate statistics
    {
      echo "=== Log Statistics ($(date)) ==="
      echo "Errors: $(wc -l < "$output_dir/errors/error.log")"
      echo "Warnings: $(wc -l < "$output_dir/warnings/warning.log")"
      echo "Critical: $(wc -l < "$output_dir/critical/critical.log")"
    } > "$output_dir/stats/current.stats"
  done
}

Large-Scale Data Processing
Advanced Data Processing Pipeline

This script processes large datasets with monitoring and error handling, ensuring efficient resource usage.
bash

#!/bin/bash
# Large dataset processing with monitoring and error handling
process_dataset() {
  local input=$1
  local output=$2
  local chunk_size=${3:-1048576} # 1MB chunks
  local max_mem=${4:-$(free -m | awk '/Mem:/ {print int($7 * 0.8)}')}

  # Create processing pipeline
  {
    # Split into manageable chunks
    split -b "$chunk_size" --filter="
    # Process each chunk
    sort -u |
    # Remove invalid entries
    grep -v '^[[:space:]]*$' |
    # Normalize data
    sed 's/[[:space:]]\+/ /g' |
    # Filter according to rules
    awk -F, '
    NF == expected_fields {
      print
    }'" "$input"
  } | \
  # Main processing pipeline
  {
    # Remove duplicates across chunks
    sort -u -S "${max_mem}M" --parallel=$(nproc) |
    # Calculate statistics
    tee >(
      awk '
      BEGIN { total = 0; count = 0 }
      {
        total += $1; count++
      }
      END {
        print "Average:", total/count
      }' > "${output}.stats"
    ) |
    # Final output
    tee "$output" |
    # Count total lines
    wc -l > "${output}.count"
  }

  # Validate output
  validate_output "$output"
}

# Monitoring function for data processing
monitor_processing() {
  local pid=$1
  local output_dir=$2

  mkdir -p "$output_dir/monitoring"

  while kill -0 "$pid" 2>/dev/null; do
    {
      echo "=== Processing Statistics ==="
      date

      # Memory usage
      ps -o pid,ppid,%cpu,%mem,vsz,rss -p "$pid"

      # I/O statistics
      iostat -x 1 1

      # System load
      uptime

      # Disk usage
      df -h "$output_dir"
    } >> "$output_dir/monitoring/stats.log"

    sleep 5
  done
}

# Data validation function
validate_output() {
  local file=$1
  local errors=0

  # Check file existence
  [[ -f "$file" ]] || {
    echo "Output file missing" >&2
    return 1
  }

  # Check file size
  [[ -s "$file" ]] || {
    echo "Output file empty" >&2
    return 1
  }

  # Validate data format
  while IFS= read -r line; do
    if ! validate_line "$line"; then
      ((errors++))
      echo "Invalid line: $line" >> "${file}.errors"
    fi
  done < "$file"

  # Report validation results
  {
    echo "=== Validation Report ==="
    echo "Total lines: $(wc -l < "$file")"
    echo "Invalid lines: $errors"
    echo "Error rate: $(bc <<< "scale=2; $errors * 100 / $(wc -l < "$file")")%"
  } > "${file}.validation"

  return $((errors > 0))
}

This guide covers advanced log management, real-time log processing and analysis, and large-scale data processing pipelines. These scripts help manage logs efficiently, process large datasets, and ensure robust error handling and monitoring.










Advanced Log Management
Robust Log Rotation System
bash

#!/bin/bash
# Advanced log management with rotation, compression, and monitoring
manage_logs() {
  local log_file=$1
  local max_size=${2:-10485760} # 10MB default
  local keep_versions=${3:-5}
  local compress_threshold=${4:-1048576} # 1MB default

  # Create log directory structure
  local log_dir=$(dirname "$log_file")
  mkdir -p "$log_dir/archive"

  # Logging function with timestamps and severity
  log() {
    local severity=$1
    local message=$2
    local timestamp=$(date -u '+%Y-%m-%dT%H:%M:%S.%3NZ')

    # Format log entry
    printf '[%s] [%s] %s\n' \
      "$timestamp" \
      "$severity" \
      "$message" >> "$log_file"

    # Check size and rotate if needed
    if [[ -f "$log_file" ]] && (($(stat -c%s "$log_file") > max_size)); then
      rotate_logs
    fi
  }

  # Log rotation with compression
  rotate_logs() {
    local timestamp=$(date '+%Y%m%d_%H%M%S')
    local archive_base="${log_dir}/archive/$(basename "$log_file")"

    # Rotate existing archives
    for ((i=keep_versions; i>0; i--)); do
      local old_archive="${archive_base}.$((i-1)).gz"
      local new_archive="${archive_base}.$i.gz"
      [[ -f "$old_archive" ]] && mv "$old_archive" "$new_archive"
    done

    # Compress current log if larger than threshold
    if [[ -f "$log_file" ]] && (($(stat -c%s "$log_file") > compress_threshold)); then
      gzip -c "$log_file" > "${archive_base}.1.gz"
    else
      mv "$log_file" "${archive_base}.1"
    fi

    # Create new log file with proper permissions
    touch "$log_file"
    chmod 0640 "$log_file"
  }

  # Log analysis function
  analyze_logs() {
    echo "=== Log Analysis ==="
    echo "Current log size: $(stat -c%s "$log_file") bytes"
    echo "Archive contents:"
    ls -lh "${log_dir}/archive/"

    echo -e "\nError frequency:"
    grep -h '\[ERROR\]' "$log_file" "${log_dir}/archive/"* | \
      sort | \
      uniq -c | \
      sort -rn | \
      head -10
  }

  # Set up monitoring
  monitor_logs() {
    local check_interval=${1:-300} # 5 minutes default
    while true; do
      local current_size=$(stat -c%s "$log_file" 2>/dev/null || echo 0)
      local used_space=$(df -P "$log_dir" | awk 'NR==2 {print $5}' | tr -d '%')
      # Alert if log growing too fast
      if ((current_size > max_size * 2)); then
        log "WARNING" "Log file growing rapidly: $current_size bytes"
      fi
      # Alert if disk space low
      if ((used_space > 90)); then
        log "ERROR" "Disk space critical: ${used_space}% used"
        rotate_logs
      fi
      sleep "$check_interval"
    done
  }

  # Start log monitor in background
  monitor_logs &
  monitor_pid=$!

  # Cleanup on exit
  trap 'kill $monitor_pid 2>/dev/null' EXIT
  return 0
}

Log Processing and Analysis
bash

#!/bin/bash
# Log processing with real-time monitoring and analysis
process_logs() {
  local input_log=$1
  local output_dir=$2
  local pattern_file=$3

  # Setup output directory
  mkdir -p "$output_dir"/{errors,warnings,critical,stats}

  # Real-time log processing
  tail -F "$input_log" | \
  while IFS= read -r line; do
    # Parse log entry
    local timestamp=$(echo "$line" | grep -oP '^\[\K[^\]]+')
    local severity=$(echo "$line" | grep -oP '\[\K(ERROR|WARNING|INFO|DEBUG|CRITICAL)[^\]]+')
    local message=${line#*]}

    # Route by severity
    case "$severity" in
      ERROR)
        echo "$line" >> "$output_dir/errors/error.log"
        notify_error "$message"
        ;;
      WARNING)
        echo "$line" >> "$output_dir/warnings/warning.log"
        ;;
      CRITICAL)
        echo "$line" >> "$output_dir/critical/critical.log"
        notify_critical "$message"
        ;;
    esac

    # Pattern matching
    if [[ -f "$pattern_file" ]]; then
      while IFS= read -r pattern; do
        if [[ "$message" =~ $pattern ]]; then
          echo "[MATCH] $line" >> "$output_dir/patterns.log"
        fi
      done < "$pattern_file"
    fi

    # Generate statistics
    {
      echo "=== Log Statistics ($(date)) ==="
      echo "Errors: $(wc -l < "$output_dir/errors/error.log")"
      echo "Warnings: $(wc -l < "$output_dir/warnings/warning.log")"
      echo "Critical: $(wc -l < "$output_dir/critical/critical.log")"
    } > "$output_dir/stats/current.stats"
  done
}

Shell Compatibility Layer
Cross-Shell Compatibility
bash

#!/bin/sh
# Portable shell script template
ensure_compatibility() {
  # Detect shell
  case "$SHELL" in
    */bash)
      shell_type="bash"
      ;;
    */zsh)
      shell_type="zsh"
      ;;
    */ksh)
      shell_type="ksh"
      ;;
    *)
      shell_type="sh"
      ;;
  esac

  # Feature detection
  check_features() {
    # Process substitution
    if ! (echo test | diff <(echo test) <(echo test) &>/dev/null); then
      echo "Process substitution not supported" >&2
      return 1
    fi

    # Here strings
    if ! echo "test" | grep test &>/dev/null; then
      echo "Here strings not supported" >&2
      return 1
    fi

    return 0
  }

  # Portable alternatives
  provide_alternatives() {
    # Instead of process substitution
    temp1=$(mktemp)
    temp2=$(mktemp)
    echo test > "$temp1"
    echo test > "$temp2"
    diff "$temp1" "$temp2"
    rm -f "$temp1" "$temp2"

    # Instead of here strings
    echo "test" | grep pattern
  }
}

# Error Handling Differences
# Portable error handling
handle_errors() {
  # Enable error handling based on shell
  case "$shell_type" in
    bash)
      set -euo pipefail
      trap 'error_handler $? $LINENO $BASH_LINENO "$BASH_COMMAND" $(printf "::%s" ${FUNCNAME[@]:-})' ERR
      ;;
    zsh)
      setopt ERR_EXIT
      setopt PIPE_FAIL
      trap 'error_handler $? $LINENO ${funcfiletrace[@]}' ERR
      ;;
    *)
      # Basic error handling for POSIX shell
      set -e
      trap 'error_handler $? $LINENO' ERR
      ;;
  esac
}

# Portable error handler
error_handler() {
  local exit_code=$1
  local line_no=$2
  shift 2
  local extra_info="$*"

  echo "Error on line $line_no (exit code $exit_code)" >&2
  [[ -n "$extra_info" ]] && echo "Additional info: $extra_info" >&2

  exit "$exit_code"
}

# Performance Considerations
# Performance optimization for different shells
optimize_performance() {
  case "$shell_type" in
    bash)
      # Bash optimizations
      enable -n coprocess # Disable coprocess if not needed
      shopt -s lastpipe # Enable last pipe optimization
      ;;
    zsh)
      # Zsh optimizations
      setopt NO_ALL_EXPORT
      setopt NO_CHECK_JOBS
      setopt NO_HUP
      ;;
    *)
      # POSIX shell optimizations
      set +m # Disable job control
      ;;
  esac
}

This guide covers advanced log management with rotation and compression, real-time log processing and analysis, handling large-scale data processing, and ensuring cross-shell compatibility with error handling and performance optimization.






























